{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='msra')\n",
    "parser.add_argument('--seed', default=1234)\n",
    "parser.add_argument('--store_dir', default=None)\n",
    "\n",
    "parser.add_argument('--max_epoch_num', default=5)\n",
    "parser.add_argument('--min_epoch_num', default=5)\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "parser.add_argument('--max_len', default=128)\n",
    "parser.add_argument('--patience', default=0.02)\n",
    "parser.add_argument('--patience_num', default=5)\n",
    "\n",
    "parser.add_argument('--full_finetuning', default=True)\n",
    "parser.add_argument('--learning_ratio', default=3e-5)\n",
    "parser.add_argument('--weight_decay', default=0.01)\n",
    "parser.add_argument('--clip_grad', default=5)\n",
    "\n",
    "parser.add_argument('--device', default=None)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_params_dir = 'experiments/' + args.dataset\n",
    "json_path = os.path.join(model_params_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "# params = utils.Params(json_path)\n",
    "\n",
    "data_dir = 'data/' + args.dataset\n",
    "if args.dataset == 'msra':\n",
    "    bert_class = 'bert-base-chinese'\n",
    "else:\n",
    "    bert_class = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f474811bd20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for random parts\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# params.seed = args.seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloader for following step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# ! pip install transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data_dir, bert_class, args, token_pad_idx=0, tag_pad_idx=1):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = args.batch_size\n",
    "        self.max_len = args.max_len\n",
    "        self.device = args.device\n",
    "        self.seed = args.seed\n",
    "        self.token_pad_idx = token_pad_idx\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        \n",
    "        tags = self.load_tags()\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "        self.idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "        \n",
    "        args.tag2idx = self.tag2idx\n",
    "        args.idx2tag = self.idx2tag\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_class, do_lower_case=False)\n",
    "        \n",
    "    def load_tags(self):\n",
    "        tags  = []\n",
    "        tags_path = os.path.join(self.data_dir, 'tags.txt')\n",
    "        \n",
    "        with open(tags_path, 'r') as file:\n",
    "            for tag in file:\n",
    "                tags.append(tag.strip())\n",
    "        return tags\n",
    "    \n",
    "    def load_sentence_tags(self, sentence_path, tags_path, data={}):\n",
    "        sentences = []\n",
    "        tags = []\n",
    "        \n",
    "        with open(sentence_path, 'r') as file:\n",
    "            for line in file:\n",
    "                tokens = line.strip().split(' ')\n",
    "                subwords = list(map(self.tokenizer.tokenize, tokens))\n",
    "                subword_lengths = list(map(len, subwords))\n",
    "                subwords = ['[CLS]'] + [item for indices in subwords for item in indices]\n",
    "                # indice words except [CLS]\n",
    "                token_start_idxs = list(range(1,len(subwords)))\n",
    "                \n",
    "                bert_tokens = self.tokenizer.convert_tokens_to_ids(subwords)\n",
    "                sentences.append((bert_tokens, token_start_idxs))\n",
    "                # len(bert_tokens) - len(token_start_idxs) = 1\n",
    "  \n",
    "                \n",
    "        if tags_path != None:\n",
    "            with open(tags_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    tag_seq = [self.tag2idx.get(tag) for tag in line.strip().split(' ')]\n",
    "                    tags.append(tag_seq)\n",
    "            \n",
    "            # Check the corresponding between sentences and tags\n",
    "            assert len(sentences) == len(tags)\n",
    "            for i in range(len(tags)):\n",
    "                assert len(tags[i]) == len(sentences[i][0])-1\n",
    "        \n",
    "        data['sentences'] = sentences\n",
    "        data['tags'] = tags\n",
    "        data['size'] = len(sentences)\n",
    "        \n",
    "    def load_data(self, data_class):\n",
    "        data = {}\n",
    "        \n",
    "        if data_class in ['train', 'val', 'test']:\n",
    "            sentence_path = os.path.join(data_dir, data_class, 'sentences.txt')\n",
    "            tags_path = os.path.join(data_dir, data_class, 'tags.txt')\n",
    "            \n",
    "            self.load_sentence_tags(sentence_path, tags_path, data)\n",
    "        \n",
    "        elif data_class == 'interactive':\n",
    "            sentence_path = os.path.join(sentence_path, data_class, 'sentences.txt')\n",
    "            tags_path=None\n",
    "            self.load_sentence_tags(sentence_path, tags_path, data)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No data in train/val/test or interactve!\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def data_iterator(self, data, shuffle=False):\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(self.seed)\n",
    "            random.shuffle(order)\n",
    "        InterModel = False if 'tags' in data else True\n",
    "        \n",
    "        if data['size'] % self.batch_size == 0:\n",
    "            BATCH_SIZE = data['size'] // self.batch_size\n",
    "        else:\n",
    "            BATCH_SIZE = data['size'] // self.batch_size + 1\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            # fetch sentences and tags\n",
    "            if i * self.batch_size < data['size'] < (i+1) * self.batch_size:\n",
    "                sentences = [data['sentences'][idx] for idx in order[i*self.batch_size:]]\n",
    "                if not InterModel:\n",
    "                    tags = [data['tags'][idx] for idx in order[i*self.batch_size:]]\n",
    "            else:\n",
    "                sentences = [data['sentences'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "                if not InterModel:\n",
    "                    tags = [data['tags'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "\n",
    "            # batch length\n",
    "            batch_len = len(sentences)\n",
    "\n",
    "            # compute length of longest sentence in batch\n",
    "            batch_max_subwords_len = max([len(s[0]) for s in sentences])\n",
    "            max_subwords_len = min(batch_max_subwords_len, self.max_len)\n",
    "            max_token_len = 0\n",
    "\n",
    "\n",
    "            # prepare a numpy array with the data, initialising the data with pad_idx\n",
    "            batch_data = self.token_pad_idx * np.ones((batch_len, max_subwords_len))\n",
    "            batch_token_starts = []\n",
    "            \n",
    "            # copy the data to the numpy array\n",
    "            for j in range(batch_len):\n",
    "                cur_subwords_len = len(sentences[j][0])\n",
    "                if cur_subwords_len <= max_subwords_len:\n",
    "                    batch_data[j][:cur_subwords_len] = sentences[j][0]\n",
    "                else:\n",
    "                    batch_data[j] = sentences[j][0][:max_subwords_len]\n",
    "                token_start_idx = sentences[j][-1]\n",
    "                token_starts = np.zeros(max_subwords_len)\n",
    "                token_starts[[idx for idx in token_start_idx if idx < max_subwords_len]] = 1\n",
    "                batch_token_starts.append(token_starts)\n",
    "                max_token_len = max(int(sum(token_starts)), max_token_len)\n",
    "            \n",
    "            if not InterModel:\n",
    "                batch_tags = self.tag_pad_idx * np.ones((batch_len, max_token_len))\n",
    "                for j in range(batch_len):\n",
    "                    cur_tags_len = len(tags[j])  \n",
    "                    if cur_tags_len <= max_token_len:\n",
    "                        batch_tags[j][:cur_tags_len] = tags[j]\n",
    "                    else:\n",
    "                        batch_tags[j] = tags[j][:max_token_len]\n",
    "            \n",
    "            # since all data are indices, we convert them to torch LongTensors\n",
    "            batch_data = torch.tensor(batch_data, dtype=torch.long)\n",
    "            batch_token_starts = torch.tensor(batch_token_starts, dtype=torch.long)\n",
    "            if not InterModel:\n",
    "                batch_tags = torch.tensor(batch_tags, dtype=torch.long)\n",
    "\n",
    "            # shift tensors to GPU if available\n",
    "            batch_data, batch_token_starts = batch_data.to(self.device), batch_token_starts.to(self.device)\n",
    "            if not InterModel:\n",
    "                batch_tags = batch_tags.to(self.device)\n",
    "                yield batch_data, batch_token_starts, batch_tags\n",
    "            else:\n",
    "                yield batch_data, batch_token_starts\n",
    "\n",
    "                \n",
    "data_loader = DataLoader(data_dir, bert_class, args, token_pad_idx=0, tag_pad_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Processing data cost: 2m 48.949304s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "load_proc_start = time.time()\n",
    "\n",
    "train = data_loader.load_data('train')\n",
    "val = data_loader.load_data('val')\n",
    "test = data_loader.load_data('test')\n",
    "load_proc_time = time.time() - load_proc_start\n",
    "lp_mins, lp_secs = load_proc_time/60, load_proc_time%60\n",
    "print('Load and Processing data cost: {0}m {1:2f}s.'.format(int(lp_mins), lp_secs))\n",
    "\n",
    "args.train_size = train['size']\n",
    "args.val_size = val['size']\n",
    "args.test_size = test['size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing Bert_CRF: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing Bert_CRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing Bert_CRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Bert_CRF were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bert_CRF(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (crf): CRF(num_tags=7)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! pip install pytorch-crf\n",
    "\n",
    "from SequenceTagger import BertForSequenceTagging, Bert_CRF\n",
    "from transformers.optimization import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "model = Bert_CRF.from_pretrained(bert_class, num_labels=len(args.tag2idx))\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuing whole model or only classifier\n",
    "if args.full_finetuning:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=args.learning_ratio,\n",
    "                  correct_bias=False)\n",
    "train_steps = args.train_size // args.batch_size\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=train_steps, num_training_steps=args.max_epoch_num * train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_entities(seq, suffix=False):\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(seq + ['O']):\n",
    "        if suffix:\n",
    "            tag = chunk[-1]\n",
    "            type_ = chunk.split('-')[0]\n",
    "        else:\n",
    "            tag = chunk[0]\n",
    "            type_ = chunk.split('-')[-1]\n",
    "\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            chunks.append((prev_type, begin_offset, i-1))\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            begin_offset = i\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred, average='micro', digits=2, suffix=False):\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = 100 * nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = 100 * nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    if any(isinstance(s, list) for s in y_true):\n",
    "        y_true = [item for sublist in y_true for item in sublist]\n",
    "        y_pred = [item for sublist in y_pred for item in sublist]\n",
    "\n",
    "    nb_correct = sum(y_t==y_p for y_t, y_p in zip(y_true, y_pred))\n",
    "    nb_true = len(y_true)\n",
    "\n",
    "    score = nb_correct / nb_true\n",
    "\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****  Train Epoch 1/5  *****\n",
      "| Train_epoch: 1 | Train_batch: 50/1335 | Train_batch_loss: 48.772859 |\n",
      "| Train_epoch: 1 | Train_batch: 100/1335 | Train_batch_loss: 31.387318 |\n",
      "| Train_epoch: 1 | Train_batch: 150/1335 | Train_batch_loss: 23.536025 |\n",
      "| Train_epoch: 1 | Train_batch: 200/1335 | Train_batch_loss: 18.847825 |\n",
      "| Train_epoch: 1 | Train_batch: 250/1335 | Train_batch_loss: 15.817019 |\n",
      "| Train_epoch: 1 | Train_batch: 300/1335 | Train_batch_loss: 13.742427 |\n",
      "| Train_epoch: 1 | Train_batch: 350/1335 | Train_batch_loss: 12.183529 |\n",
      "| Train_epoch: 1 | Train_batch: 400/1335 | Train_batch_loss: 11.015920 |\n",
      "| Train_epoch: 1 | Train_batch: 450/1335 | Train_batch_loss: 10.058633 |\n",
      "| Train_epoch: 1 | Train_batch: 500/1335 | Train_batch_loss: 9.279172 |\n",
      "| Train_epoch: 1 | Train_batch: 550/1335 | Train_batch_loss: 8.638265 |\n",
      "| Train_epoch: 1 | Train_batch: 600/1335 | Train_batch_loss: 8.076138 |\n",
      "| Train_epoch: 1 | Train_batch: 650/1335 | Train_batch_loss: 7.633831 |\n",
      "| Train_epoch: 1 | Train_batch: 700/1335 | Train_batch_loss: 7.228841 |\n",
      "| Train_epoch: 1 | Train_batch: 750/1335 | Train_batch_loss: 6.868888 |\n",
      "| Train_epoch: 1 | Train_batch: 800/1335 | Train_batch_loss: 6.558018 |\n",
      "| Train_epoch: 1 | Train_batch: 850/1335 | Train_batch_loss: 6.269029 |\n",
      "| Train_epoch: 1 | Train_batch: 900/1335 | Train_batch_loss: 6.025908 |\n",
      "| Train_epoch: 1 | Train_batch: 950/1335 | Train_batch_loss: 5.801233 |\n",
      "| Train_epoch: 1 | Train_batch: 1000/1335 | Train_batch_loss: 5.595479 |\n",
      "| Train_epoch: 1 | Train_batch: 1050/1335 | Train_batch_loss: 5.405368 |\n",
      "| Train_epoch: 1 | Train_batch: 1100/1335 | Train_batch_loss: 5.228412 |\n",
      "| Train_epoch: 1 | Train_batch: 1150/1335 | Train_batch_loss: 5.068734 |\n",
      "| Train_epoch: 1 | Train_batch: 1200/1335 | Train_batch_loss: 4.918083 |\n",
      "| Train_epoch: 1 | Train_batch: 1250/1335 | Train_batch_loss: 4.782259 |\n",
      "| Train_epoch: 1 | Train_batch: 1300/1335 | Train_batch_loss: 4.651989 |\n",
      "| Train_epoch: 1 | Train_loss: 4.573548 | Train&Val_time: 10m 57s |\n",
      "                 | Valid_loss: 1.150421 | Valid_f1: 92.674942 | Valid_Acc: 99.260854% |\n",
      "\n",
      "*****  Train Epoch 2/5  *****\n",
      "| Train_epoch: 2 | Train_batch: 50/1335 | Train_batch_loss: 1.436277 |\n",
      "| Train_epoch: 2 | Train_batch: 100/1335 | Train_batch_loss: 1.421782 |\n",
      "| Train_epoch: 2 | Train_batch: 150/1335 | Train_batch_loss: 1.445691 |\n",
      "| Train_epoch: 2 | Train_batch: 200/1335 | Train_batch_loss: 1.450822 |\n",
      "| Train_epoch: 2 | Train_batch: 250/1335 | Train_batch_loss: 1.419307 |\n",
      "| Train_epoch: 2 | Train_batch: 300/1335 | Train_batch_loss: 1.423493 |\n",
      "| Train_epoch: 2 | Train_batch: 350/1335 | Train_batch_loss: 1.376993 |\n",
      "| Train_epoch: 2 | Train_batch: 400/1335 | Train_batch_loss: 1.335045 |\n",
      "| Train_epoch: 2 | Train_batch: 450/1335 | Train_batch_loss: 1.320563 |\n",
      "| Train_epoch: 2 | Train_batch: 500/1335 | Train_batch_loss: 1.291225 |\n",
      "| Train_epoch: 2 | Train_batch: 550/1335 | Train_batch_loss: 1.265364 |\n",
      "| Train_epoch: 2 | Train_batch: 600/1335 | Train_batch_loss: 1.235817 |\n",
      "| Train_epoch: 2 | Train_batch: 650/1335 | Train_batch_loss: 1.232726 |\n",
      "| Train_epoch: 2 | Train_batch: 700/1335 | Train_batch_loss: 1.206742 |\n",
      "| Train_epoch: 2 | Train_batch: 750/1335 | Train_batch_loss: 1.185075 |\n",
      "| Train_epoch: 2 | Train_batch: 800/1335 | Train_batch_loss: 1.171817 |\n",
      "| Train_epoch: 2 | Train_batch: 850/1335 | Train_batch_loss: 1.149029 |\n",
      "| Train_epoch: 2 | Train_batch: 900/1335 | Train_batch_loss: 1.144976 |\n",
      "| Train_epoch: 2 | Train_batch: 950/1335 | Train_batch_loss: 1.128943 |\n",
      "| Train_epoch: 2 | Train_batch: 1000/1335 | Train_batch_loss: 1.113756 |\n",
      "| Train_epoch: 2 | Train_batch: 1050/1335 | Train_batch_loss: 1.092518 |\n",
      "| Train_epoch: 2 | Train_batch: 1100/1335 | Train_batch_loss: 1.075716 |\n",
      "| Train_epoch: 2 | Train_batch: 1150/1335 | Train_batch_loss: 1.059157 |\n",
      "| Train_epoch: 2 | Train_batch: 1200/1335 | Train_batch_loss: 1.042475 |\n",
      "| Train_epoch: 2 | Train_batch: 1250/1335 | Train_batch_loss: 1.024083 |\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# ! pip install pytorch-crf\n",
    "from torchcrf import CRF\n",
    "\n",
    "model_crf = CRF(len(args.tag2idx))\n",
    "\n",
    "if args.store_dir is not None:\n",
    "    model = BertForSequenceTagging.from_pretrained(model_params_dir)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, args.max_epoch_num +1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print('\\n*****',' Train Epoch {0}/{1} '.format(epoch, args.max_epoch_num), '*****')\n",
    "    \n",
    "    train_steps = args.train_size // args.batch_size\n",
    "    val_steps = args.val_size // args.batch_size\n",
    "    \n",
    "    train_data_iterator = data_loader.data_iterator(train, shuffle=True)\n",
    "    val_data_iterator = data_loader.data_iterator(val, shuffle=True)\n",
    "    \n",
    "    # Train step\n",
    "    model.train()\n",
    "    epoch_loss, epoch_avg_loss = 0.0, 0.0\n",
    "\n",
    "    for batch in range(1, train_steps+1):\n",
    "        batch_data, batch_token_starts, batch_tags = next(train_data_iterator)\n",
    "        batch_masks = batch_data.gt(0)\n",
    "        loss = model(batch_data[:,1:], attn_masks=batch_masks[:,1:], labels=batch_tags)\n",
    "                    \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=args.clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_avg_loss = epoch_loss / (batch+1)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print('| Train_epoch: {0} | Train_batch: {1}/{2} | Train_batch_loss: {3:4f} |'.format(epoch, batch, train_steps, epoch_avg_loss))\n",
    "    \n",
    "    # Val step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    true_tags, pred_tags = [], []\n",
    "    \n",
    "    for _ in range(val_steps):\n",
    "        batch_data, batch_token_starts, batch_tags = next(val_data_iterator)\n",
    "        batch_masks = batch_data.gt(0)\n",
    "        \n",
    "        loss = model(batch_data[:,1:], attn_masks=batch_masks[:,1:], labels=batch_tags)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        batch_outs = model(batch_data[:,1:], attn_masks=batch_masks[:,1:])\n",
    "        batch_tags = batch_tags.to('cpu').numpy()\n",
    "        \n",
    "#         print(len(batch_outs), len(batch_outs[1]), len(batch_outs[2]))\n",
    "#         print(batch_tags.shape)\n",
    "        \n",
    "        pred_tags.extend([[args.idx2tag.get(idx) for idx in indices] for indices in batch_outs])\n",
    "        for indices in batch_tags:\n",
    "            tmp = []\n",
    "            for idx in indices:\n",
    "                if idx != -1:\n",
    "                    tmp.append(args.idx2tag.get(idx))\n",
    "                else:\n",
    "                    pass\n",
    "            true_tags.append(tmp)\n",
    "    \n",
    "    assert len(pred_tags) == len(true_tags)\n",
    "\n",
    "    f1 = f1_score(true_tags, pred_tags)\n",
    "    loss = val_loss / val_steps\n",
    "    acc = accuracy_score(true_tags, pred_tags)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_mins, epoch_secs = int(epoch_time/60), int(epoch_time%60)\n",
    "    \n",
    "    print('| Train_epoch: {0} | Train_loss: {2:4f} | Train&Val_time: {3}m {4}s |'.format(epoch, args.max_epoch_num, epoch_avg_loss, epoch_mins, epoch_secs))\n",
    "    print('                 | Valid_loss: {0:4f} | Valid_f1: {1:4f} | Valid_Acc: {2:2f}% |'.format(loss, f1, 100*acc))\n",
    "\n",
    "    if f1 - best_val_f1 >= 0.1:\n",
    "        best_val_f1 = f1\n",
    "        \n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        model_path = model_params_dir + '/pytorch_model.bin'\n",
    "        config_path = model_params_dir + '/config.json'\n",
    "        \n",
    "        torch.save(model_to_save.state_dict(), model_path, _use_new_zipfile_serialization=False)\n",
    "        model_to_save.config.to_json_file(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
