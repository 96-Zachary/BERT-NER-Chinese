{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='msra')\n",
    "parser.add_argument('--seed', default=1234)\n",
    "parser.add_argument('--store_dir', default=None)\n",
    "\n",
    "parser.add_argument('--max_epoch_num', default=20)\n",
    "parser.add_argument('--min_epoch_num', default=5)\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "parser.add_argument('--max_len', default=128)\n",
    "parser.add_argument('--patience', default=0.02)\n",
    "parser.add_argument('--patience_num', default=5)\n",
    "\n",
    "parser.add_argument('--full_finetuning', default=True)\n",
    "parser.add_argument('--learning_ratio', default=3e-5)\n",
    "parser.add_argument('--weight_decay', default=0.01)\n",
    "parser.add_argument('--clip_grad', default=5)\n",
    "\n",
    "parser.add_argument('--device', default=None)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_params_dir = 'experiments/' + args.dataset\n",
    "json_path = os.path.join(model_params_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "# params = utils.Params(json_path)\n",
    "\n",
    "data_dir = 'data/' + args.dataset\n",
    "if args.dataset == 'msra':\n",
    "    bert_class = 'bert-base-chinese'\n",
    "else:\n",
    "    bert_class = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f90f2620d20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for random parts\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# params.seed = args.seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloader for following step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# ! pip install transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data_dir, bert_class, args, token_pad_idx=0, tag_pad_idx=1):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = args.batch_size\n",
    "        self.max_len = args.max_len\n",
    "        self.device = args.device\n",
    "        self.seed = args.seed\n",
    "        self.token_pad_idx = token_pad_idx\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        \n",
    "        tags = self.load_tags()\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "        self.idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "        \n",
    "        args.tag2idx = self.tag2idx\n",
    "        args.idx2tag = self.idx2tag\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_class, do_lower_case=False)\n",
    "        \n",
    "    def load_tags(self):\n",
    "        tags  = []\n",
    "        tags_path = os.path.join(self.data_dir, 'tags.txt')\n",
    "        \n",
    "        with open(tags_path, 'r') as file:\n",
    "            for tag in file:\n",
    "                tags.append(tag.strip())\n",
    "        return tags\n",
    "    \n",
    "    def load_sentence_tags(self, sentence_path, tags_path, data={}):\n",
    "        sentences = []\n",
    "        tags = []\n",
    "        \n",
    "        with open(sentence_path, 'r') as file:\n",
    "            for line in file:\n",
    "                tokens = line.strip().split(' ')\n",
    "                subwords = list(map(self.tokenizer.tokenize, tokens))\n",
    "                subword_lengths = list(map(len, subwords))\n",
    "                subwords = ['[CLS]'] + [item for indices in subwords for item in indices]\n",
    "                # indice words except [CLS]\n",
    "                token_start_idxs = list(range(1,len(subwords)))\n",
    "                \n",
    "                bert_tokens = self.tokenizer.convert_tokens_to_ids(subwords)\n",
    "                sentences.append((bert_tokens, token_start_idxs))\n",
    "                # len(bert_tokens) - len(token_start_idxs) = 1\n",
    "  \n",
    "                \n",
    "        if tags_path != None:\n",
    "            with open(tags_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    tag_seq = [self.tag2idx.get(tag) for tag in line.strip().split(' ')]\n",
    "                    tags.append(tag_seq)\n",
    "            \n",
    "            # Check the corresponding between sentences and tags\n",
    "            assert len(sentences) == len(tags)\n",
    "            for i in range(len(tags)):\n",
    "                assert len(tags[i]) == len(sentences[i][0])-1\n",
    "        \n",
    "        data['sentences'] = sentences\n",
    "        data['tags'] = tags\n",
    "        data['size'] = len(sentences)\n",
    "        \n",
    "    def load_data(self, data_class):\n",
    "        data = {}\n",
    "        \n",
    "        if data_class in ['train', 'val', 'test']:\n",
    "            sentence_path = os.path.join(data_dir, data_class, 'sentences.txt')\n",
    "            tags_path = os.path.join(data_dir, data_class, 'tags.txt')\n",
    "            \n",
    "            self.load_sentence_tags(sentence_path, tags_path, data)\n",
    "        \n",
    "        elif data_class == 'interactive':\n",
    "            sentence_path = os.path.join(sentence_path, data_class, 'sentences.txt')\n",
    "            tags_path=None\n",
    "            self.load_sentence_tags(sentence_path, tags_path, data)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No data in train/val/test or interactve!\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def data_iterator(self, data, shuffle=False):\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(self.seed)\n",
    "            random.shuffle(order)\n",
    "        InterModel = False if 'tags' in data else True\n",
    "        \n",
    "        if data['size'] % self.batch_size == 0:\n",
    "            BATCH_SIZE = data['size'] // self.batch_size\n",
    "        else:\n",
    "            BATCH_SIZE = data['size'] // self.batch_size + 1\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            # fetch sentences and tags\n",
    "            if i * self.batch_size < data['size'] < (i+1) * self.batch_size:\n",
    "                sentences = [data['sentences'][idx] for idx in order[i*self.batch_size:]]\n",
    "                if not InterModel:\n",
    "                    tags = [data['tags'][idx] for idx in order[i*self.batch_size:]]\n",
    "            else:\n",
    "                sentences = [data['sentences'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "                if not InterModel:\n",
    "                    tags = [data['tags'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "\n",
    "            # batch length\n",
    "            batch_len = len(sentences)\n",
    "\n",
    "            # compute length of longest sentence in batch\n",
    "            batch_max_subwords_len = max([len(s[0]) for s in sentences])\n",
    "            max_subwords_len = min(batch_max_subwords_len, self.max_len)\n",
    "            max_token_len = 0\n",
    "\n",
    "\n",
    "            # prepare a numpy array with the data, initialising the data with pad_idx\n",
    "            batch_data = self.token_pad_idx * np.ones((batch_len, max_subwords_len))\n",
    "            batch_token_starts = []\n",
    "            \n",
    "            # copy the data to the numpy array\n",
    "            for j in range(batch_len):\n",
    "                cur_subwords_len = len(sentences[j][0])\n",
    "                if cur_subwords_len <= max_subwords_len:\n",
    "                    batch_data[j][:cur_subwords_len] = sentences[j][0]\n",
    "                else:\n",
    "                    batch_data[j] = sentences[j][0][:max_subwords_len]\n",
    "                token_start_idx = sentences[j][-1]\n",
    "                token_starts = np.zeros(max_subwords_len)\n",
    "                token_starts[[idx for idx in token_start_idx if idx < max_subwords_len]] = 1\n",
    "                batch_token_starts.append(token_starts)\n",
    "                max_token_len = max(int(sum(token_starts)), max_token_len)\n",
    "            \n",
    "            if not InterModel:\n",
    "                batch_tags = self.tag_pad_idx * np.ones((batch_len, max_token_len))\n",
    "                for j in range(batch_len):\n",
    "                    cur_tags_len = len(tags[j])  \n",
    "                    if cur_tags_len <= max_token_len:\n",
    "                        batch_tags[j][:cur_tags_len] = tags[j]\n",
    "                    else:\n",
    "                        batch_tags[j] = tags[j][:max_token_len]\n",
    "            \n",
    "            # since all data are indices, we convert them to torch LongTensors\n",
    "            batch_data = torch.tensor(batch_data, dtype=torch.long)\n",
    "            batch_token_starts = torch.tensor(batch_token_starts, dtype=torch.long)\n",
    "            if not InterModel:\n",
    "                batch_tags = torch.tensor(batch_tags, dtype=torch.long)\n",
    "\n",
    "            # shift tensors to GPU if available\n",
    "            batch_data, batch_token_starts = batch_data.to(self.device), batch_token_starts.to(self.device)\n",
    "            if not InterModel:\n",
    "                batch_tags = batch_tags.to(self.device)\n",
    "                yield batch_data, batch_token_starts, batch_tags\n",
    "            else:\n",
    "                yield batch_data, batch_token_starts\n",
    "\n",
    "                \n",
    "data_loader = DataLoader(data_dir, bert_class, args, token_pad_idx=0, tag_pad_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Processing data cost: 1m 56.925149s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "load_proc_start = time.time()\n",
    "\n",
    "train = data_loader.load_data('train')\n",
    "val = data_loader.load_data('val')\n",
    "test = data_loader.load_data('test')\n",
    "load_proc_time = time.time() - load_proc_start\n",
    "lp_mins, lp_secs = load_proc_time/60, load_proc_time%60\n",
    "print('Load and Processing data cost: {0}m {1:2f}s.'.format(int(lp_mins), lp_secs))\n",
    "\n",
    "args.train_size = train['size']\n",
    "args.val_size = val['size']\n",
    "args.test_size = test['size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceTagging: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceTagging from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceTagging from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceTagging were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceTagging(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SequenceTagger import BertForSequenceTagging\n",
    "from transformers.optimization import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "model = BertForSequenceTagging.from_pretrained(bert_class, num_labels=len(args.tag2idx))\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuing whole model or only classifier\n",
    "if args.full_finetuning:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=args.learning_ratio,\n",
    "                  correct_bias=False)\n",
    "train_steps = args.train_size // args.batch_size\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=train_steps, num_training_steps=args.max_epoch_num * train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_entities(seq, suffix=False):\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(seq + ['O']):\n",
    "        if suffix:\n",
    "            tag = chunk[-1]\n",
    "            type_ = chunk.split('-')[0]\n",
    "        else:\n",
    "            tag = chunk[0]\n",
    "            type_ = chunk.split('-')[-1]\n",
    "\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            chunks.append((prev_type, begin_offset, i-1))\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            begin_offset = i\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred, average='micro', digits=2, suffix=False):\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = 100 * nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = 100 * nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    if any(isinstance(s, list) for s in y_true):\n",
    "        y_true = [item for sublist in y_true for item in sublist]\n",
    "        y_pred = [item for sublist in y_pred for item in sublist]\n",
    "\n",
    "    nb_correct = sum(y_t==y_p for y_t, y_p in zip(y_true, y_pred))\n",
    "    nb_true = len(y_true)\n",
    "\n",
    "    score = nb_correct / nb_true\n",
    "\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****  Train Epoch 1/20  *****\n",
      "| Train_epoch: 1 | Train_batch: 50/1335 | Train_batch_loss: 1.167071 |\n",
      "| Train_epoch: 1 | Train_batch: 100/1335 | Train_batch_loss: 0.758920 |\n",
      "| Train_epoch: 1 | Train_batch: 150/1335 | Train_batch_loss: 0.561824 |\n",
      "| Train_epoch: 1 | Train_batch: 200/1335 | Train_batch_loss: 0.444074 |\n",
      "| Train_epoch: 1 | Train_batch: 250/1335 | Train_batch_loss: 0.368825 |\n",
      "| Train_epoch: 1 | Train_batch: 300/1335 | Train_batch_loss: 0.318133 |\n",
      "| Train_epoch: 1 | Train_batch: 350/1335 | Train_batch_loss: 0.280247 |\n",
      "| Train_epoch: 1 | Train_batch: 400/1335 | Train_batch_loss: 0.251750 |\n",
      "| Train_epoch: 1 | Train_batch: 450/1335 | Train_batch_loss: 0.228844 |\n",
      "| Train_epoch: 1 | Train_batch: 500/1335 | Train_batch_loss: 0.209940 |\n",
      "| Train_epoch: 1 | Train_batch: 550/1335 | Train_batch_loss: 0.194619 |\n",
      "| Train_epoch: 1 | Train_batch: 600/1335 | Train_batch_loss: 0.181661 |\n",
      "| Train_epoch: 1 | Train_batch: 650/1335 | Train_batch_loss: 0.171312 |\n",
      "| Train_epoch: 1 | Train_batch: 700/1335 | Train_batch_loss: 0.161635 |\n",
      "| Train_epoch: 1 | Train_batch: 750/1335 | Train_batch_loss: 0.153151 |\n",
      "| Train_epoch: 1 | Train_batch: 800/1335 | Train_batch_loss: 0.145636 |\n",
      "| Train_epoch: 1 | Train_batch: 850/1335 | Train_batch_loss: 0.138805 |\n",
      "| Train_epoch: 1 | Train_batch: 900/1335 | Train_batch_loss: 0.133259 |\n",
      "| Train_epoch: 1 | Train_batch: 950/1335 | Train_batch_loss: 0.128154 |\n",
      "| Train_epoch: 1 | Train_batch: 1000/1335 | Train_batch_loss: 0.123549 |\n",
      "| Train_epoch: 1 | Train_batch: 1050/1335 | Train_batch_loss: 0.119216 |\n",
      "| Train_epoch: 1 | Train_batch: 1100/1335 | Train_batch_loss: 0.114977 |\n",
      "| Train_epoch: 1 | Train_batch: 1150/1335 | Train_batch_loss: 0.111200 |\n",
      "| Train_epoch: 1 | Train_batch: 1200/1335 | Train_batch_loss: 0.107808 |\n",
      "| Train_epoch: 1 | Train_batch: 1250/1335 | Train_batch_loss: 0.104599 |\n",
      "| Train_epoch: 1 | Train_batch: 1300/1335 | Train_batch_loss: 0.101623 |\n",
      "| Train_epoch: 1 | Train_loss: 0.099750 | Train&Val_time: 7m 43s |\n",
      "                 | Valid_loss: 0.023811 | Valid_f1: 92.563117 | Valid_Acc: 99.691285% |\n",
      "\n",
      "*****  Train Epoch 2/20  *****\n",
      "| Train_epoch: 2 | Train_batch: 50/1335 | Train_batch_loss: 0.030852 |\n",
      "| Train_epoch: 2 | Train_batch: 100/1335 | Train_batch_loss: 0.029306 |\n",
      "| Train_epoch: 2 | Train_batch: 150/1335 | Train_batch_loss: 0.029261 |\n",
      "| Train_epoch: 2 | Train_batch: 200/1335 | Train_batch_loss: 0.028059 |\n",
      "| Train_epoch: 2 | Train_batch: 250/1335 | Train_batch_loss: 0.026975 |\n",
      "| Train_epoch: 2 | Train_batch: 300/1335 | Train_batch_loss: 0.026810 |\n",
      "| Train_epoch: 2 | Train_batch: 350/1335 | Train_batch_loss: 0.026142 |\n",
      "| Train_epoch: 2 | Train_batch: 400/1335 | Train_batch_loss: 0.025427 |\n",
      "| Train_epoch: 2 | Train_batch: 450/1335 | Train_batch_loss: 0.025198 |\n",
      "| Train_epoch: 2 | Train_batch: 500/1335 | Train_batch_loss: 0.024801 |\n",
      "| Train_epoch: 2 | Train_batch: 550/1335 | Train_batch_loss: 0.024381 |\n",
      "| Train_epoch: 2 | Train_batch: 600/1335 | Train_batch_loss: 0.023802 |\n",
      "| Train_epoch: 2 | Train_batch: 650/1335 | Train_batch_loss: 0.023578 |\n",
      "| Train_epoch: 2 | Train_batch: 700/1335 | Train_batch_loss: 0.023173 |\n",
      "| Train_epoch: 2 | Train_batch: 750/1335 | Train_batch_loss: 0.022685 |\n",
      "| Train_epoch: 2 | Train_batch: 800/1335 | Train_batch_loss: 0.022381 |\n",
      "| Train_epoch: 2 | Train_batch: 850/1335 | Train_batch_loss: 0.021932 |\n",
      "| Train_epoch: 2 | Train_batch: 900/1335 | Train_batch_loss: 0.021888 |\n",
      "| Train_epoch: 2 | Train_batch: 950/1335 | Train_batch_loss: 0.021629 |\n",
      "| Train_epoch: 2 | Train_batch: 1000/1335 | Train_batch_loss: 0.021420 |\n",
      "| Train_epoch: 2 | Train_batch: 1050/1335 | Train_batch_loss: 0.021077 |\n",
      "| Train_epoch: 2 | Train_batch: 1100/1335 | Train_batch_loss: 0.020654 |\n",
      "| Train_epoch: 2 | Train_batch: 1150/1335 | Train_batch_loss: 0.020494 |\n",
      "| Train_epoch: 2 | Train_batch: 1200/1335 | Train_batch_loss: 0.020305 |\n",
      "| Train_epoch: 2 | Train_batch: 1250/1335 | Train_batch_loss: 0.020052 |\n",
      "| Train_epoch: 2 | Train_batch: 1300/1335 | Train_batch_loss: 0.019723 |\n",
      "| Train_epoch: 2 | Train_loss: 0.019532 | Train&Val_time: 7m 45s |\n",
      "                 | Valid_loss: 0.021928 | Valid_f1: 94.205862 | Valid_Acc: 99.747272% |\n",
      "\n",
      "*****  Train Epoch 3/20  *****\n",
      "| Train_epoch: 3 | Train_batch: 50/1335 | Train_batch_loss: 0.013630 |\n",
      "| Train_epoch: 3 | Train_batch: 100/1335 | Train_batch_loss: 0.012945 |\n",
      "| Train_epoch: 3 | Train_batch: 150/1335 | Train_batch_loss: 0.013852 |\n",
      "| Train_epoch: 3 | Train_batch: 200/1335 | Train_batch_loss: 0.013433 |\n",
      "| Train_epoch: 3 | Train_batch: 250/1335 | Train_batch_loss: 0.013403 |\n",
      "| Train_epoch: 3 | Train_batch: 300/1335 | Train_batch_loss: 0.013171 |\n",
      "| Train_epoch: 3 | Train_batch: 350/1335 | Train_batch_loss: 0.013319 |\n",
      "| Train_epoch: 3 | Train_batch: 400/1335 | Train_batch_loss: 0.012887 |\n",
      "| Train_epoch: 3 | Train_batch: 450/1335 | Train_batch_loss: 0.012960 |\n",
      "| Train_epoch: 3 | Train_batch: 500/1335 | Train_batch_loss: 0.012803 |\n",
      "| Train_epoch: 3 | Train_batch: 550/1335 | Train_batch_loss: 0.012597 |\n",
      "| Train_epoch: 3 | Train_batch: 600/1335 | Train_batch_loss: 0.012592 |\n",
      "| Train_epoch: 3 | Train_batch: 650/1335 | Train_batch_loss: 0.012651 |\n",
      "| Train_epoch: 3 | Train_batch: 700/1335 | Train_batch_loss: 0.012529 |\n",
      "| Train_epoch: 3 | Train_batch: 750/1335 | Train_batch_loss: 0.012409 |\n",
      "| Train_epoch: 3 | Train_batch: 800/1335 | Train_batch_loss: 0.012154 |\n",
      "| Train_epoch: 3 | Train_batch: 850/1335 | Train_batch_loss: 0.011872 |\n",
      "| Train_epoch: 3 | Train_batch: 900/1335 | Train_batch_loss: 0.011880 |\n",
      "| Train_epoch: 3 | Train_batch: 950/1335 | Train_batch_loss: 0.011805 |\n",
      "| Train_epoch: 3 | Train_batch: 1000/1335 | Train_batch_loss: 0.011703 |\n",
      "| Train_epoch: 3 | Train_batch: 1050/1335 | Train_batch_loss: 0.011577 |\n",
      "| Train_epoch: 3 | Train_batch: 1100/1335 | Train_batch_loss: 0.011349 |\n",
      "| Train_epoch: 3 | Train_batch: 1150/1335 | Train_batch_loss: 0.011233 |\n",
      "| Train_epoch: 3 | Train_batch: 1200/1335 | Train_batch_loss: 0.011249 |\n",
      "| Train_epoch: 3 | Train_batch: 1250/1335 | Train_batch_loss: 0.011133 |\n",
      "| Train_epoch: 3 | Train_batch: 1300/1335 | Train_batch_loss: 0.011020 |\n",
      "| Train_epoch: 3 | Train_loss: 0.010924 | Train&Val_time: 7m 43s |\n",
      "                 | Valid_loss: 0.028017 | Valid_f1: 94.273248 | Valid_Acc: 99.721250% |\n",
      "\n",
      "*****  Train Epoch 4/20  *****\n",
      "| Train_epoch: 4 | Train_batch: 50/1335 | Train_batch_loss: 0.008795 |\n",
      "| Train_epoch: 4 | Train_batch: 100/1335 | Train_batch_loss: 0.008937 |\n",
      "| Train_epoch: 4 | Train_batch: 150/1335 | Train_batch_loss: 0.008357 |\n",
      "| Train_epoch: 4 | Train_batch: 200/1335 | Train_batch_loss: 0.007916 |\n",
      "| Train_epoch: 4 | Train_batch: 250/1335 | Train_batch_loss: 0.007940 |\n",
      "| Train_epoch: 4 | Train_batch: 300/1335 | Train_batch_loss: 0.007787 |\n",
      "| Train_epoch: 4 | Train_batch: 350/1335 | Train_batch_loss: 0.007867 |\n",
      "| Train_epoch: 4 | Train_batch: 400/1335 | Train_batch_loss: 0.007733 |\n",
      "| Train_epoch: 4 | Train_batch: 450/1335 | Train_batch_loss: 0.007897 |\n",
      "| Train_epoch: 4 | Train_batch: 500/1335 | Train_batch_loss: 0.007990 |\n",
      "| Train_epoch: 4 | Train_batch: 550/1335 | Train_batch_loss: 0.007986 |\n",
      "| Train_epoch: 4 | Train_batch: 600/1335 | Train_batch_loss: 0.007873 |\n",
      "| Train_epoch: 4 | Train_batch: 650/1335 | Train_batch_loss: 0.007981 |\n",
      "| Train_epoch: 4 | Train_batch: 700/1335 | Train_batch_loss: 0.007938 |\n",
      "| Train_epoch: 4 | Train_batch: 750/1335 | Train_batch_loss: 0.007873 |\n",
      "| Train_epoch: 4 | Train_batch: 800/1335 | Train_batch_loss: 0.007802 |\n",
      "| Train_epoch: 4 | Train_batch: 850/1335 | Train_batch_loss: 0.007726 |\n",
      "| Train_epoch: 4 | Train_batch: 900/1335 | Train_batch_loss: 0.007725 |\n",
      "| Train_epoch: 4 | Train_batch: 950/1335 | Train_batch_loss: 0.007660 |\n",
      "| Train_epoch: 4 | Train_batch: 1000/1335 | Train_batch_loss: 0.007605 |\n",
      "| Train_epoch: 4 | Train_batch: 1050/1335 | Train_batch_loss: 0.007515 |\n",
      "| Train_epoch: 4 | Train_batch: 1100/1335 | Train_batch_loss: 0.007454 |\n",
      "| Train_epoch: 4 | Train_batch: 1150/1335 | Train_batch_loss: 0.007368 |\n",
      "| Train_epoch: 4 | Train_batch: 1200/1335 | Train_batch_loss: 0.007274 |\n",
      "| Train_epoch: 4 | Train_batch: 1250/1335 | Train_batch_loss: 0.007270 |\n",
      "| Train_epoch: 4 | Train_batch: 1300/1335 | Train_batch_loss: 0.007188 |\n",
      "| Train_epoch: 4 | Train_loss: 0.007106 | Train&Val_time: 7m 43s |\n",
      "                 | Valid_loss: 0.027283 | Valid_f1: 95.191257 | Valid_Acc: 99.750820% |\n",
      "\n",
      "*****  Train Epoch 5/20  *****\n",
      "| Train_epoch: 5 | Train_batch: 50/1335 | Train_batch_loss: 0.004820 |\n",
      "| Train_epoch: 5 | Train_batch: 100/1335 | Train_batch_loss: 0.004368 |\n",
      "| Train_epoch: 5 | Train_batch: 150/1335 | Train_batch_loss: 0.005330 |\n",
      "| Train_epoch: 5 | Train_batch: 200/1335 | Train_batch_loss: 0.005302 |\n",
      "| Train_epoch: 5 | Train_batch: 250/1335 | Train_batch_loss: 0.005658 |\n",
      "| Train_epoch: 5 | Train_batch: 300/1335 | Train_batch_loss: 0.005843 |\n",
      "| Train_epoch: 5 | Train_batch: 350/1335 | Train_batch_loss: 0.005951 |\n",
      "| Train_epoch: 5 | Train_batch: 400/1335 | Train_batch_loss: 0.005896 |\n",
      "| Train_epoch: 5 | Train_batch: 450/1335 | Train_batch_loss: 0.005853 |\n",
      "| Train_epoch: 5 | Train_batch: 500/1335 | Train_batch_loss: 0.005723 |\n",
      "| Train_epoch: 5 | Train_batch: 550/1335 | Train_batch_loss: 0.005764 |\n",
      "| Train_epoch: 5 | Train_batch: 600/1335 | Train_batch_loss: 0.005674 |\n",
      "| Train_epoch: 5 | Train_batch: 650/1335 | Train_batch_loss: 0.005692 |\n",
      "| Train_epoch: 5 | Train_batch: 700/1335 | Train_batch_loss: 0.005685 |\n",
      "| Train_epoch: 5 | Train_batch: 750/1335 | Train_batch_loss: 0.005677 |\n",
      "| Train_epoch: 5 | Train_batch: 800/1335 | Train_batch_loss: 0.005571 |\n",
      "| Train_epoch: 5 | Train_batch: 850/1335 | Train_batch_loss: 0.005538 |\n",
      "| Train_epoch: 5 | Train_batch: 900/1335 | Train_batch_loss: 0.005548 |\n",
      "| Train_epoch: 5 | Train_batch: 950/1335 | Train_batch_loss: 0.005554 |\n",
      "| Train_epoch: 5 | Train_batch: 1000/1335 | Train_batch_loss: 0.005695 |\n",
      "| Train_epoch: 5 | Train_batch: 1050/1335 | Train_batch_loss: 0.005675 |\n",
      "| Train_epoch: 5 | Train_batch: 1100/1335 | Train_batch_loss: 0.005585 |\n",
      "| Train_epoch: 5 | Train_batch: 1150/1335 | Train_batch_loss: 0.005611 |\n",
      "| Train_epoch: 5 | Train_batch: 1200/1335 | Train_batch_loss: 0.005718 |\n",
      "| Train_epoch: 5 | Train_batch: 1250/1335 | Train_batch_loss: 0.005682 |\n",
      "| Train_epoch: 5 | Train_batch: 1300/1335 | Train_batch_loss: 0.005626 |\n",
      "| Train_epoch: 5 | Train_loss: 0.005630 | Train&Val_time: 7m 46s |\n",
      "                 | Valid_loss: 0.027450 | Valid_f1: 95.331695 | Valid_Acc: 99.758706% |\n",
      "\n",
      "*****  Train Epoch 6/20  *****\n",
      "| Train_epoch: 6 | Train_batch: 50/1335 | Train_batch_loss: 0.005645 |\n",
      "| Train_epoch: 6 | Train_batch: 100/1335 | Train_batch_loss: 0.004274 |\n",
      "| Train_epoch: 6 | Train_batch: 150/1335 | Train_batch_loss: 0.004542 |\n",
      "| Train_epoch: 6 | Train_batch: 200/1335 | Train_batch_loss: 0.004848 |\n",
      "| Train_epoch: 6 | Train_batch: 250/1335 | Train_batch_loss: 0.004643 |\n",
      "| Train_epoch: 6 | Train_batch: 300/1335 | Train_batch_loss: 0.004591 |\n",
      "| Train_epoch: 6 | Train_batch: 350/1335 | Train_batch_loss: 0.004701 |\n",
      "| Train_epoch: 6 | Train_batch: 400/1335 | Train_batch_loss: 0.004696 |\n",
      "| Train_epoch: 6 | Train_batch: 450/1335 | Train_batch_loss: 0.004665 |\n",
      "| Train_epoch: 6 | Train_batch: 500/1335 | Train_batch_loss: 0.004631 |\n",
      "| Train_epoch: 6 | Train_batch: 550/1335 | Train_batch_loss: 0.004582 |\n",
      "| Train_epoch: 6 | Train_batch: 600/1335 | Train_batch_loss: 0.004559 |\n",
      "| Train_epoch: 6 | Train_batch: 650/1335 | Train_batch_loss: 0.004543 |\n",
      "| Train_epoch: 6 | Train_batch: 700/1335 | Train_batch_loss: 0.004516 |\n",
      "| Train_epoch: 6 | Train_batch: 750/1335 | Train_batch_loss: 0.004430 |\n",
      "| Train_epoch: 6 | Train_batch: 800/1335 | Train_batch_loss: 0.004312 |\n",
      "| Train_epoch: 6 | Train_batch: 850/1335 | Train_batch_loss: 0.004287 |\n",
      "| Train_epoch: 6 | Train_batch: 900/1335 | Train_batch_loss: 0.004260 |\n",
      "| Train_epoch: 6 | Train_batch: 950/1335 | Train_batch_loss: 0.004313 |\n",
      "| Train_epoch: 6 | Train_batch: 1000/1335 | Train_batch_loss: 0.004299 |\n",
      "| Train_epoch: 6 | Train_batch: 1050/1335 | Train_batch_loss: 0.004363 |\n",
      "| Train_epoch: 6 | Train_batch: 1100/1335 | Train_batch_loss: 0.004289 |\n",
      "| Train_epoch: 6 | Train_batch: 1150/1335 | Train_batch_loss: 0.004221 |\n",
      "| Train_epoch: 6 | Train_batch: 1200/1335 | Train_batch_loss: 0.004212 |\n",
      "| Train_epoch: 6 | Train_batch: 1250/1335 | Train_batch_loss: 0.004197 |\n",
      "| Train_epoch: 6 | Train_batch: 1300/1335 | Train_batch_loss: 0.004154 |\n",
      "| Train_epoch: 6 | Train_loss: 0.004171 | Train&Val_time: 7m 45s |\n",
      "                 | Valid_loss: 0.027102 | Valid_f1: 95.889285 | Valid_Acc: 99.785516% |\n",
      "\n",
      "*****  Train Epoch 7/20  *****\n",
      "| Train_epoch: 7 | Train_batch: 50/1335 | Train_batch_loss: 0.005656 |\n",
      "| Train_epoch: 7 | Train_batch: 100/1335 | Train_batch_loss: 0.004375 |\n",
      "| Train_epoch: 7 | Train_batch: 150/1335 | Train_batch_loss: 0.003931 |\n",
      "| Train_epoch: 7 | Train_batch: 200/1335 | Train_batch_loss: 0.003881 |\n",
      "| Train_epoch: 7 | Train_batch: 250/1335 | Train_batch_loss: 0.003873 |\n",
      "| Train_epoch: 7 | Train_batch: 300/1335 | Train_batch_loss: 0.003829 |\n",
      "| Train_epoch: 7 | Train_batch: 350/1335 | Train_batch_loss: 0.003834 |\n",
      "| Train_epoch: 7 | Train_batch: 400/1335 | Train_batch_loss: 0.003578 |\n",
      "| Train_epoch: 7 | Train_batch: 450/1335 | Train_batch_loss: 0.003847 |\n",
      "| Train_epoch: 7 | Train_batch: 500/1335 | Train_batch_loss: 0.003933 |\n",
      "| Train_epoch: 7 | Train_batch: 550/1335 | Train_batch_loss: 0.003950 |\n",
      "| Train_epoch: 7 | Train_batch: 600/1335 | Train_batch_loss: 0.003906 |\n",
      "| Train_epoch: 7 | Train_batch: 650/1335 | Train_batch_loss: 0.003830 |\n",
      "| Train_epoch: 7 | Train_batch: 700/1335 | Train_batch_loss: 0.003716 |\n",
      "| Train_epoch: 7 | Train_batch: 750/1335 | Train_batch_loss: 0.003652 |\n",
      "| Train_epoch: 7 | Train_batch: 800/1335 | Train_batch_loss: 0.003595 |\n",
      "| Train_epoch: 7 | Train_batch: 850/1335 | Train_batch_loss: 0.003505 |\n",
      "| Train_epoch: 7 | Train_batch: 900/1335 | Train_batch_loss: 0.003533 |\n",
      "| Train_epoch: 7 | Train_batch: 950/1335 | Train_batch_loss: 0.003509 |\n",
      "| Train_epoch: 7 | Train_batch: 1000/1335 | Train_batch_loss: 0.003579 |\n",
      "| Train_epoch: 7 | Train_batch: 1050/1335 | Train_batch_loss: 0.003527 |\n",
      "| Train_epoch: 7 | Train_batch: 1100/1335 | Train_batch_loss: 0.003476 |\n",
      "| Train_epoch: 7 | Train_batch: 1150/1335 | Train_batch_loss: 0.003477 |\n",
      "| Train_epoch: 7 | Train_batch: 1200/1335 | Train_batch_loss: 0.003394 |\n",
      "| Train_epoch: 7 | Train_batch: 1250/1335 | Train_batch_loss: 0.003401 |\n",
      "| Train_epoch: 7 | Train_batch: 1300/1335 | Train_batch_loss: 0.003390 |\n",
      "| Train_epoch: 7 | Train_loss: 0.003471 | Train&Val_time: 7m 44s |\n",
      "                 | Valid_loss: 0.024027 | Valid_f1: 95.078821 | Valid_Acc: 99.752003% |\n",
      "\n",
      "*****  Train Epoch 8/20  *****\n",
      "| Train_epoch: 8 | Train_batch: 50/1335 | Train_batch_loss: 0.002474 |\n",
      "| Train_epoch: 8 | Train_batch: 100/1335 | Train_batch_loss: 0.002573 |\n",
      "| Train_epoch: 8 | Train_batch: 150/1335 | Train_batch_loss: 0.002563 |\n",
      "| Train_epoch: 8 | Train_batch: 200/1335 | Train_batch_loss: 0.002641 |\n",
      "| Train_epoch: 8 | Train_batch: 250/1335 | Train_batch_loss: 0.002730 |\n",
      "| Train_epoch: 8 | Train_batch: 300/1335 | Train_batch_loss: 0.002758 |\n",
      "| Train_epoch: 8 | Train_batch: 350/1335 | Train_batch_loss: 0.002776 |\n",
      "| Train_epoch: 8 | Train_batch: 400/1335 | Train_batch_loss: 0.002793 |\n",
      "| Train_epoch: 8 | Train_batch: 450/1335 | Train_batch_loss: 0.002984 |\n",
      "| Train_epoch: 8 | Train_batch: 500/1335 | Train_batch_loss: 0.002982 |\n",
      "| Train_epoch: 8 | Train_batch: 550/1335 | Train_batch_loss: 0.002903 |\n",
      "| Train_epoch: 8 | Train_batch: 600/1335 | Train_batch_loss: 0.002807 |\n",
      "| Train_epoch: 8 | Train_batch: 650/1335 | Train_batch_loss: 0.002721 |\n",
      "| Train_epoch: 8 | Train_batch: 700/1335 | Train_batch_loss: 0.002775 |\n",
      "| Train_epoch: 8 | Train_batch: 750/1335 | Train_batch_loss: 0.002786 |\n",
      "| Train_epoch: 8 | Train_batch: 800/1335 | Train_batch_loss: 0.002741 |\n",
      "| Train_epoch: 8 | Train_batch: 850/1335 | Train_batch_loss: 0.002714 |\n",
      "| Train_epoch: 8 | Train_batch: 900/1335 | Train_batch_loss: 0.002715 |\n",
      "| Train_epoch: 8 | Train_batch: 950/1335 | Train_batch_loss: 0.002658 |\n",
      "| Train_epoch: 8 | Train_batch: 1000/1335 | Train_batch_loss: 0.002615 |\n",
      "| Train_epoch: 8 | Train_batch: 1050/1335 | Train_batch_loss: 0.002555 |\n",
      "| Train_epoch: 8 | Train_batch: 1100/1335 | Train_batch_loss: 0.002514 |\n",
      "| Train_epoch: 8 | Train_batch: 1150/1335 | Train_batch_loss: 0.002461 |\n",
      "| Train_epoch: 8 | Train_batch: 1200/1335 | Train_batch_loss: 0.002468 |\n",
      "| Train_epoch: 8 | Train_batch: 1250/1335 | Train_batch_loss: 0.002527 |\n",
      "| Train_epoch: 8 | Train_batch: 1300/1335 | Train_batch_loss: 0.002569 |\n",
      "| Train_epoch: 8 | Train_loss: 0.002559 | Train&Val_time: 7m 44s |\n",
      "                 | Valid_loss: 0.025692 | Valid_f1: 95.548555 | Valid_Acc: 99.793007% |\n",
      "\n",
      "*****  Train Epoch 9/20  *****\n",
      "| Train_epoch: 9 | Train_batch: 50/1335 | Train_batch_loss: 0.002686 |\n",
      "| Train_epoch: 9 | Train_batch: 100/1335 | Train_batch_loss: 0.002597 |\n",
      "| Train_epoch: 9 | Train_batch: 150/1335 | Train_batch_loss: 0.002651 |\n",
      "| Train_epoch: 9 | Train_batch: 200/1335 | Train_batch_loss: 0.002845 |\n",
      "| Train_epoch: 9 | Train_batch: 250/1335 | Train_batch_loss: 0.003125 |\n",
      "| Train_epoch: 9 | Train_batch: 300/1335 | Train_batch_loss: 0.002991 |\n",
      "| Train_epoch: 9 | Train_batch: 350/1335 | Train_batch_loss: 0.002771 |\n",
      "| Train_epoch: 9 | Train_batch: 400/1335 | Train_batch_loss: 0.002661 |\n",
      "| Train_epoch: 9 | Train_batch: 450/1335 | Train_batch_loss: 0.002745 |\n",
      "| Train_epoch: 9 | Train_batch: 500/1335 | Train_batch_loss: 0.002661 |\n",
      "| Train_epoch: 9 | Train_batch: 550/1335 | Train_batch_loss: 0.002544 |\n",
      "| Train_epoch: 9 | Train_batch: 600/1335 | Train_batch_loss: 0.002488 |\n",
      "| Train_epoch: 9 | Train_batch: 650/1335 | Train_batch_loss: 0.002395 |\n",
      "| Train_epoch: 9 | Train_batch: 700/1335 | Train_batch_loss: 0.002375 |\n",
      "| Train_epoch: 9 | Train_batch: 750/1335 | Train_batch_loss: 0.002319 |\n",
      "| Train_epoch: 9 | Train_batch: 800/1335 | Train_batch_loss: 0.002253 |\n",
      "| Train_epoch: 9 | Train_batch: 850/1335 | Train_batch_loss: 0.002266 |\n",
      "| Train_epoch: 9 | Train_batch: 900/1335 | Train_batch_loss: 0.002229 |\n",
      "| Train_epoch: 9 | Train_batch: 950/1335 | Train_batch_loss: 0.002200 |\n",
      "| Train_epoch: 9 | Train_batch: 1000/1335 | Train_batch_loss: 0.002175 |\n",
      "| Train_epoch: 9 | Train_batch: 1050/1335 | Train_batch_loss: 0.002162 |\n",
      "| Train_epoch: 9 | Train_batch: 1100/1335 | Train_batch_loss: 0.002140 |\n",
      "| Train_epoch: 9 | Train_batch: 1150/1335 | Train_batch_loss: 0.002104 |\n",
      "| Train_epoch: 9 | Train_batch: 1200/1335 | Train_batch_loss: 0.002069 |\n",
      "| Train_epoch: 9 | Train_batch: 1250/1335 | Train_batch_loss: 0.002041 |\n",
      "| Train_epoch: 9 | Train_batch: 1300/1335 | Train_batch_loss: 0.002042 |\n",
      "| Train_epoch: 9 | Train_loss: 0.002036 | Train&Val_time: 7m 42s |\n",
      "                 | Valid_loss: 0.030987 | Valid_f1: 95.430218 | Valid_Acc: 99.755551% |\n",
      "\n",
      "*****  Train Epoch 10/20  *****\n",
      "| Train_epoch: 10 | Train_batch: 50/1335 | Train_batch_loss: 0.001391 |\n",
      "| Train_epoch: 10 | Train_batch: 100/1335 | Train_batch_loss: 0.001355 |\n",
      "| Train_epoch: 10 | Train_batch: 150/1335 | Train_batch_loss: 0.001831 |\n",
      "| Train_epoch: 10 | Train_batch: 200/1335 | Train_batch_loss: 0.001814 |\n",
      "| Train_epoch: 10 | Train_batch: 250/1335 | Train_batch_loss: 0.001721 |\n",
      "| Train_epoch: 10 | Train_batch: 300/1335 | Train_batch_loss: 0.001874 |\n",
      "| Train_epoch: 10 | Train_batch: 350/1335 | Train_batch_loss: 0.001867 |\n",
      "| Train_epoch: 10 | Train_batch: 400/1335 | Train_batch_loss: 0.001873 |\n",
      "| Train_epoch: 10 | Train_batch: 450/1335 | Train_batch_loss: 0.001937 |\n",
      "| Train_epoch: 10 | Train_batch: 500/1335 | Train_batch_loss: 0.001959 |\n",
      "| Train_epoch: 10 | Train_batch: 550/1335 | Train_batch_loss: 0.001918 |\n",
      "| Train_epoch: 10 | Train_batch: 600/1335 | Train_batch_loss: 0.001874 |\n",
      "| Train_epoch: 10 | Train_batch: 650/1335 | Train_batch_loss: 0.001838 |\n",
      "| Train_epoch: 10 | Train_batch: 700/1335 | Train_batch_loss: 0.001870 |\n",
      "| Train_epoch: 10 | Train_batch: 750/1335 | Train_batch_loss: 0.001841 |\n",
      "| Train_epoch: 10 | Train_batch: 800/1335 | Train_batch_loss: 0.001778 |\n",
      "| Train_epoch: 10 | Train_batch: 850/1335 | Train_batch_loss: 0.001758 |\n",
      "| Train_epoch: 10 | Train_batch: 900/1335 | Train_batch_loss: 0.001704 |\n",
      "| Train_epoch: 10 | Train_batch: 950/1335 | Train_batch_loss: 0.001684 |\n",
      "| Train_epoch: 10 | Train_batch: 1000/1335 | Train_batch_loss: 0.001669 |\n",
      "| Train_epoch: 10 | Train_batch: 1050/1335 | Train_batch_loss: 0.001639 |\n",
      "| Train_epoch: 10 | Train_batch: 1100/1335 | Train_batch_loss: 0.001632 |\n",
      "| Train_epoch: 10 | Train_batch: 1150/1335 | Train_batch_loss: 0.001621 |\n",
      "| Train_epoch: 10 | Train_batch: 1200/1335 | Train_batch_loss: 0.001614 |\n",
      "| Train_epoch: 10 | Train_batch: 1250/1335 | Train_batch_loss: 0.001614 |\n",
      "| Train_epoch: 10 | Train_batch: 1300/1335 | Train_batch_loss: 0.001608 |\n",
      "| Train_epoch: 10 | Train_loss: 0.001595 | Train&Val_time: 7m 47s |\n",
      "                 | Valid_loss: 0.029468 | Valid_f1: 96.451569 | Valid_Acc: 99.806018% |\n",
      "\n",
      "*****  Train Epoch 11/20  *****\n",
      "| Train_epoch: 11 | Train_batch: 50/1335 | Train_batch_loss: 0.001391 |\n",
      "| Train_epoch: 11 | Train_batch: 100/1335 | Train_batch_loss: 0.001183 |\n",
      "| Train_epoch: 11 | Train_batch: 150/1335 | Train_batch_loss: 0.001077 |\n",
      "| Train_epoch: 11 | Train_batch: 200/1335 | Train_batch_loss: 0.001226 |\n",
      "| Train_epoch: 11 | Train_batch: 250/1335 | Train_batch_loss: 0.001752 |\n",
      "| Train_epoch: 11 | Train_batch: 300/1335 | Train_batch_loss: 0.001762 |\n",
      "| Train_epoch: 11 | Train_batch: 350/1335 | Train_batch_loss: 0.001854 |\n",
      "| Train_epoch: 11 | Train_batch: 400/1335 | Train_batch_loss: 0.001747 |\n",
      "| Train_epoch: 11 | Train_batch: 450/1335 | Train_batch_loss: 0.001738 |\n",
      "| Train_epoch: 11 | Train_batch: 500/1335 | Train_batch_loss: 0.001863 |\n",
      "| Train_epoch: 11 | Train_batch: 550/1335 | Train_batch_loss: 0.001784 |\n",
      "| Train_epoch: 11 | Train_batch: 600/1335 | Train_batch_loss: 0.001751 |\n",
      "| Train_epoch: 11 | Train_batch: 650/1335 | Train_batch_loss: 0.001740 |\n",
      "| Train_epoch: 11 | Train_batch: 700/1335 | Train_batch_loss: 0.001711 |\n",
      "| Train_epoch: 11 | Train_batch: 750/1335 | Train_batch_loss: 0.001677 |\n",
      "| Train_epoch: 11 | Train_batch: 800/1335 | Train_batch_loss: 0.001675 |\n",
      "| Train_epoch: 11 | Train_batch: 850/1335 | Train_batch_loss: 0.001617 |\n",
      "| Train_epoch: 11 | Train_batch: 900/1335 | Train_batch_loss: 0.001561 |\n",
      "| Train_epoch: 11 | Train_batch: 950/1335 | Train_batch_loss: 0.001545 |\n",
      "| Train_epoch: 11 | Train_batch: 1000/1335 | Train_batch_loss: 0.001492 |\n",
      "| Train_epoch: 11 | Train_batch: 1050/1335 | Train_batch_loss: 0.001465 |\n",
      "| Train_epoch: 11 | Train_batch: 1100/1335 | Train_batch_loss: 0.001448 |\n",
      "| Train_epoch: 11 | Train_batch: 1150/1335 | Train_batch_loss: 0.001450 |\n",
      "| Train_epoch: 11 | Train_batch: 1200/1335 | Train_batch_loss: 0.001433 |\n",
      "| Train_epoch: 11 | Train_batch: 1250/1335 | Train_batch_loss: 0.001396 |\n",
      "| Train_epoch: 11 | Train_batch: 1300/1335 | Train_batch_loss: 0.001407 |\n",
      "| Train_epoch: 11 | Train_loss: 0.001397 | Train&Val_time: 7m 52s |\n",
      "                 | Valid_loss: 0.031196 | Valid_f1: 96.173961 | Valid_Acc: 99.807201% |\n",
      "\n",
      "*****  Train Epoch 12/20  *****\n",
      "| Train_epoch: 12 | Train_batch: 50/1335 | Train_batch_loss: 0.001159 |\n",
      "| Train_epoch: 12 | Train_batch: 100/1335 | Train_batch_loss: 0.001464 |\n",
      "| Train_epoch: 12 | Train_batch: 150/1335 | Train_batch_loss: 0.001511 |\n",
      "| Train_epoch: 12 | Train_batch: 200/1335 | Train_batch_loss: 0.001548 |\n",
      "| Train_epoch: 12 | Train_batch: 250/1335 | Train_batch_loss: 0.001470 |\n",
      "| Train_epoch: 12 | Train_batch: 300/1335 | Train_batch_loss: 0.001372 |\n",
      "| Train_epoch: 12 | Train_batch: 350/1335 | Train_batch_loss: 0.001303 |\n",
      "| Train_epoch: 12 | Train_batch: 400/1335 | Train_batch_loss: 0.001221 |\n",
      "| Train_epoch: 12 | Train_batch: 450/1335 | Train_batch_loss: 0.001187 |\n",
      "| Train_epoch: 12 | Train_batch: 500/1335 | Train_batch_loss: 0.001153 |\n",
      "| Train_epoch: 12 | Train_batch: 550/1335 | Train_batch_loss: 0.001201 |\n",
      "| Train_epoch: 12 | Train_batch: 600/1335 | Train_batch_loss: 0.001156 |\n",
      "| Train_epoch: 12 | Train_batch: 650/1335 | Train_batch_loss: 0.001126 |\n",
      "| Train_epoch: 12 | Train_batch: 700/1335 | Train_batch_loss: 0.001127 |\n",
      "| Train_epoch: 12 | Train_batch: 750/1335 | Train_batch_loss: 0.001114 |\n",
      "| Train_epoch: 12 | Train_batch: 800/1335 | Train_batch_loss: 0.001083 |\n",
      "| Train_epoch: 12 | Train_batch: 850/1335 | Train_batch_loss: 0.001102 |\n",
      "| Train_epoch: 12 | Train_batch: 900/1335 | Train_batch_loss: 0.001084 |\n",
      "| Train_epoch: 12 | Train_batch: 950/1335 | Train_batch_loss: 0.001072 |\n",
      "| Train_epoch: 12 | Train_batch: 1000/1335 | Train_batch_loss: 0.001065 |\n",
      "| Train_epoch: 12 | Train_batch: 1050/1335 | Train_batch_loss: 0.001118 |\n",
      "| Train_epoch: 12 | Train_batch: 1100/1335 | Train_batch_loss: 0.001150 |\n",
      "| Train_epoch: 12 | Train_batch: 1150/1335 | Train_batch_loss: 0.001134 |\n",
      "| Train_epoch: 12 | Train_batch: 1200/1335 | Train_batch_loss: 0.001118 |\n",
      "| Train_epoch: 12 | Train_batch: 1250/1335 | Train_batch_loss: 0.001103 |\n",
      "| Train_epoch: 12 | Train_batch: 1300/1335 | Train_batch_loss: 0.001087 |\n",
      "| Train_epoch: 12 | Train_loss: 0.001082 | Train&Val_time: 7m 54s |\n",
      "                 | Valid_loss: 0.033881 | Valid_f1: 95.765741 | Valid_Acc: 99.772899% |\n",
      "\n",
      "*****  Train Epoch 13/20  *****\n",
      "| Train_epoch: 13 | Train_batch: 50/1335 | Train_batch_loss: 0.000587 |\n",
      "| Train_epoch: 13 | Train_batch: 100/1335 | Train_batch_loss: 0.000550 |\n",
      "| Train_epoch: 13 | Train_batch: 150/1335 | Train_batch_loss: 0.000900 |\n",
      "| Train_epoch: 13 | Train_batch: 200/1335 | Train_batch_loss: 0.001046 |\n",
      "| Train_epoch: 13 | Train_batch: 250/1335 | Train_batch_loss: 0.001036 |\n",
      "| Train_epoch: 13 | Train_batch: 300/1335 | Train_batch_loss: 0.000996 |\n",
      "| Train_epoch: 13 | Train_batch: 350/1335 | Train_batch_loss: 0.001025 |\n",
      "| Train_epoch: 13 | Train_batch: 400/1335 | Train_batch_loss: 0.000988 |\n",
      "| Train_epoch: 13 | Train_batch: 450/1335 | Train_batch_loss: 0.000907 |\n",
      "| Train_epoch: 13 | Train_batch: 500/1335 | Train_batch_loss: 0.000891 |\n",
      "| Train_epoch: 13 | Train_batch: 550/1335 | Train_batch_loss: 0.000923 |\n",
      "| Train_epoch: 13 | Train_batch: 600/1335 | Train_batch_loss: 0.000909 |\n",
      "| Train_epoch: 13 | Train_batch: 650/1335 | Train_batch_loss: 0.000914 |\n",
      "| Train_epoch: 13 | Train_batch: 700/1335 | Train_batch_loss: 0.000913 |\n",
      "| Train_epoch: 13 | Train_batch: 750/1335 | Train_batch_loss: 0.000926 |\n",
      "| Train_epoch: 13 | Train_batch: 800/1335 | Train_batch_loss: 0.000899 |\n",
      "| Train_epoch: 13 | Train_batch: 850/1335 | Train_batch_loss: 0.000877 |\n",
      "| Train_epoch: 13 | Train_batch: 900/1335 | Train_batch_loss: 0.000870 |\n",
      "| Train_epoch: 13 | Train_batch: 950/1335 | Train_batch_loss: 0.000906 |\n",
      "| Train_epoch: 13 | Train_batch: 1000/1335 | Train_batch_loss: 0.000888 |\n",
      "| Train_epoch: 13 | Train_batch: 1050/1335 | Train_batch_loss: 0.000865 |\n",
      "| Train_epoch: 13 | Train_batch: 1100/1335 | Train_batch_loss: 0.000885 |\n",
      "| Train_epoch: 13 | Train_batch: 1150/1335 | Train_batch_loss: 0.000884 |\n",
      "| Train_epoch: 13 | Train_batch: 1200/1335 | Train_batch_loss: 0.000899 |\n",
      "| Train_epoch: 13 | Train_batch: 1250/1335 | Train_batch_loss: 0.000924 |\n",
      "| Train_epoch: 13 | Train_batch: 1300/1335 | Train_batch_loss: 0.000920 |\n",
      "| Train_epoch: 13 | Train_loss: 0.000909 | Train&Val_time: 7m 58s |\n",
      "                 | Valid_loss: 0.032214 | Valid_f1: 95.794393 | Valid_Acc: 99.776053% |\n",
      "\n",
      "*****  Train Epoch 14/20  *****\n",
      "| Train_epoch: 14 | Train_batch: 50/1335 | Train_batch_loss: 0.000410 |\n",
      "| Train_epoch: 14 | Train_batch: 100/1335 | Train_batch_loss: 0.000333 |\n",
      "| Train_epoch: 14 | Train_batch: 150/1335 | Train_batch_loss: 0.000389 |\n",
      "| Train_epoch: 14 | Train_batch: 200/1335 | Train_batch_loss: 0.000587 |\n",
      "| Train_epoch: 14 | Train_batch: 250/1335 | Train_batch_loss: 0.000611 |\n",
      "| Train_epoch: 14 | Train_batch: 300/1335 | Train_batch_loss: 0.000739 |\n",
      "| Train_epoch: 14 | Train_batch: 350/1335 | Train_batch_loss: 0.000711 |\n",
      "| Train_epoch: 14 | Train_batch: 400/1335 | Train_batch_loss: 0.000657 |\n",
      "| Train_epoch: 14 | Train_batch: 450/1335 | Train_batch_loss: 0.000629 |\n",
      "| Train_epoch: 14 | Train_batch: 500/1335 | Train_batch_loss: 0.000636 |\n",
      "| Train_epoch: 14 | Train_batch: 550/1335 | Train_batch_loss: 0.000607 |\n",
      "| Train_epoch: 14 | Train_batch: 600/1335 | Train_batch_loss: 0.000653 |\n",
      "| Train_epoch: 14 | Train_batch: 650/1335 | Train_batch_loss: 0.000664 |\n",
      "| Train_epoch: 14 | Train_batch: 700/1335 | Train_batch_loss: 0.000651 |\n",
      "| Train_epoch: 14 | Train_batch: 750/1335 | Train_batch_loss: 0.000675 |\n",
      "| Train_epoch: 14 | Train_batch: 800/1335 | Train_batch_loss: 0.000666 |\n",
      "| Train_epoch: 14 | Train_batch: 850/1335 | Train_batch_loss: 0.000706 |\n",
      "| Train_epoch: 14 | Train_batch: 900/1335 | Train_batch_loss: 0.000683 |\n",
      "| Train_epoch: 14 | Train_batch: 950/1335 | Train_batch_loss: 0.000675 |\n",
      "| Train_epoch: 14 | Train_batch: 1000/1335 | Train_batch_loss: 0.000664 |\n",
      "| Train_epoch: 14 | Train_batch: 1050/1335 | Train_batch_loss: 0.000654 |\n",
      "| Train_epoch: 14 | Train_batch: 1100/1335 | Train_batch_loss: 0.000642 |\n",
      "| Train_epoch: 14 | Train_batch: 1150/1335 | Train_batch_loss: 0.000625 |\n",
      "| Train_epoch: 14 | Train_batch: 1200/1335 | Train_batch_loss: 0.000611 |\n",
      "| Train_epoch: 14 | Train_batch: 1250/1335 | Train_batch_loss: 0.000603 |\n",
      "| Train_epoch: 14 | Train_batch: 1300/1335 | Train_batch_loss: 0.000586 |\n",
      "| Train_epoch: 14 | Train_loss: 0.000574 | Train&Val_time: 8m 7s |\n",
      "                 | Valid_loss: 0.034801 | Valid_f1: 95.976933 | Valid_Acc: 99.778419% |\n",
      "\n",
      "*****  Train Epoch 15/20  *****\n",
      "| Train_epoch: 15 | Train_batch: 50/1335 | Train_batch_loss: 0.000309 |\n",
      "| Train_epoch: 15 | Train_batch: 100/1335 | Train_batch_loss: 0.000293 |\n",
      "| Train_epoch: 15 | Train_batch: 150/1335 | Train_batch_loss: 0.000476 |\n",
      "| Train_epoch: 15 | Train_batch: 200/1335 | Train_batch_loss: 0.000430 |\n",
      "| Train_epoch: 15 | Train_batch: 250/1335 | Train_batch_loss: 0.000479 |\n",
      "| Train_epoch: 15 | Train_batch: 300/1335 | Train_batch_loss: 0.000466 |\n",
      "| Train_epoch: 15 | Train_batch: 350/1335 | Train_batch_loss: 0.000490 |\n",
      "| Train_epoch: 15 | Train_batch: 400/1335 | Train_batch_loss: 0.000462 |\n",
      "| Train_epoch: 15 | Train_batch: 450/1335 | Train_batch_loss: 0.000514 |\n",
      "| Train_epoch: 15 | Train_batch: 500/1335 | Train_batch_loss: 0.000508 |\n",
      "| Train_epoch: 15 | Train_batch: 550/1335 | Train_batch_loss: 0.000509 |\n",
      "| Train_epoch: 15 | Train_batch: 600/1335 | Train_batch_loss: 0.000511 |\n",
      "| Train_epoch: 15 | Train_batch: 650/1335 | Train_batch_loss: 0.000494 |\n",
      "| Train_epoch: 15 | Train_batch: 700/1335 | Train_batch_loss: 0.000500 |\n",
      "| Train_epoch: 15 | Train_batch: 750/1335 | Train_batch_loss: 0.000492 |\n",
      "| Train_epoch: 15 | Train_batch: 800/1335 | Train_batch_loss: 0.000481 |\n",
      "| Train_epoch: 15 | Train_batch: 850/1335 | Train_batch_loss: 0.000478 |\n",
      "| Train_epoch: 15 | Train_batch: 900/1335 | Train_batch_loss: 0.000478 |\n",
      "| Train_epoch: 15 | Train_batch: 950/1335 | Train_batch_loss: 0.000473 |\n",
      "| Train_epoch: 15 | Train_batch: 1000/1335 | Train_batch_loss: 0.000463 |\n",
      "| Train_epoch: 15 | Train_batch: 1050/1335 | Train_batch_loss: 0.000479 |\n",
      "| Train_epoch: 15 | Train_batch: 1100/1335 | Train_batch_loss: 0.000489 |\n",
      "| Train_epoch: 15 | Train_batch: 1150/1335 | Train_batch_loss: 0.000491 |\n",
      "| Train_epoch: 15 | Train_batch: 1200/1335 | Train_batch_loss: 0.000479 |\n",
      "| Train_epoch: 15 | Train_batch: 1250/1335 | Train_batch_loss: 0.000466 |\n",
      "| Train_epoch: 15 | Train_batch: 1300/1335 | Train_batch_loss: 0.000475 |\n",
      "| Train_epoch: 15 | Train_loss: 0.000478 | Train&Val_time: 8m 0s |\n",
      "                 | Valid_loss: 0.031924 | Valid_f1: 95.959318 | Valid_Acc: 99.791036% |\n",
      "\n",
      "*****  Train Epoch 16/20  *****\n",
      "| Train_epoch: 16 | Train_batch: 50/1335 | Train_batch_loss: 0.000226 |\n",
      "| Train_epoch: 16 | Train_batch: 100/1335 | Train_batch_loss: 0.000259 |\n",
      "| Train_epoch: 16 | Train_batch: 150/1335 | Train_batch_loss: 0.000376 |\n",
      "| Train_epoch: 16 | Train_batch: 200/1335 | Train_batch_loss: 0.000419 |\n",
      "| Train_epoch: 16 | Train_batch: 250/1335 | Train_batch_loss: 0.000414 |\n",
      "| Train_epoch: 16 | Train_batch: 300/1335 | Train_batch_loss: 0.000402 |\n",
      "| Train_epoch: 16 | Train_batch: 350/1335 | Train_batch_loss: 0.000381 |\n",
      "| Train_epoch: 16 | Train_batch: 400/1335 | Train_batch_loss: 0.000345 |\n",
      "| Train_epoch: 16 | Train_batch: 450/1335 | Train_batch_loss: 0.000338 |\n",
      "| Train_epoch: 16 | Train_batch: 500/1335 | Train_batch_loss: 0.000321 |\n",
      "| Train_epoch: 16 | Train_batch: 550/1335 | Train_batch_loss: 0.000315 |\n",
      "| Train_epoch: 16 | Train_batch: 600/1335 | Train_batch_loss: 0.000322 |\n",
      "| Train_epoch: 16 | Train_batch: 650/1335 | Train_batch_loss: 0.000347 |\n",
      "| Train_epoch: 16 | Train_batch: 700/1335 | Train_batch_loss: 0.000373 |\n",
      "| Train_epoch: 16 | Train_batch: 750/1335 | Train_batch_loss: 0.000362 |\n",
      "| Train_epoch: 16 | Train_batch: 800/1335 | Train_batch_loss: 0.000351 |\n",
      "| Train_epoch: 16 | Train_batch: 850/1335 | Train_batch_loss: 0.000351 |\n",
      "| Train_epoch: 16 | Train_batch: 900/1335 | Train_batch_loss: 0.000353 |\n",
      "| Train_epoch: 16 | Train_batch: 950/1335 | Train_batch_loss: 0.000354 |\n",
      "| Train_epoch: 16 | Train_batch: 1000/1335 | Train_batch_loss: 0.000347 |\n",
      "| Train_epoch: 16 | Train_batch: 1050/1335 | Train_batch_loss: 0.000348 |\n",
      "| Train_epoch: 16 | Train_batch: 1100/1335 | Train_batch_loss: 0.000358 |\n",
      "| Train_epoch: 16 | Train_batch: 1150/1335 | Train_batch_loss: 0.000362 |\n",
      "| Train_epoch: 16 | Train_batch: 1200/1335 | Train_batch_loss: 0.000373 |\n",
      "| Train_epoch: 16 | Train_batch: 1250/1335 | Train_batch_loss: 0.000376 |\n",
      "| Train_epoch: 16 | Train_batch: 1300/1335 | Train_batch_loss: 0.000371 |\n",
      "| Train_epoch: 16 | Train_loss: 0.000363 | Train&Val_time: 8m 8s |\n",
      "                 | Valid_loss: 0.035091 | Valid_f1: 96.053713 | Valid_Acc: 99.794190% |\n",
      "\n",
      "*****  Train Epoch 17/20  *****\n",
      "| Train_epoch: 17 | Train_batch: 50/1335 | Train_batch_loss: 0.000239 |\n",
      "| Train_epoch: 17 | Train_batch: 100/1335 | Train_batch_loss: 0.000280 |\n",
      "| Train_epoch: 17 | Train_batch: 150/1335 | Train_batch_loss: 0.000256 |\n",
      "| Train_epoch: 17 | Train_batch: 200/1335 | Train_batch_loss: 0.000208 |\n",
      "| Train_epoch: 17 | Train_batch: 250/1335 | Train_batch_loss: 0.000199 |\n",
      "| Train_epoch: 17 | Train_batch: 300/1335 | Train_batch_loss: 0.000281 |\n",
      "| Train_epoch: 17 | Train_batch: 350/1335 | Train_batch_loss: 0.000269 |\n",
      "| Train_epoch: 17 | Train_batch: 400/1335 | Train_batch_loss: 0.000348 |\n",
      "| Train_epoch: 17 | Train_batch: 450/1335 | Train_batch_loss: 0.000338 |\n",
      "| Train_epoch: 17 | Train_batch: 500/1335 | Train_batch_loss: 0.000313 |\n",
      "| Train_epoch: 17 | Train_batch: 550/1335 | Train_batch_loss: 0.000307 |\n",
      "| Train_epoch: 17 | Train_batch: 600/1335 | Train_batch_loss: 0.000309 |\n",
      "| Train_epoch: 17 | Train_batch: 650/1335 | Train_batch_loss: 0.000313 |\n",
      "| Train_epoch: 17 | Train_batch: 700/1335 | Train_batch_loss: 0.000313 |\n",
      "| Train_epoch: 17 | Train_batch: 750/1335 | Train_batch_loss: 0.000312 |\n",
      "| Train_epoch: 17 | Train_batch: 800/1335 | Train_batch_loss: 0.000307 |\n",
      "| Train_epoch: 17 | Train_batch: 850/1335 | Train_batch_loss: 0.000293 |\n",
      "| Train_epoch: 17 | Train_batch: 900/1335 | Train_batch_loss: 0.000299 |\n",
      "| Train_epoch: 17 | Train_batch: 950/1335 | Train_batch_loss: 0.000304 |\n",
      "| Train_epoch: 17 | Train_batch: 1000/1335 | Train_batch_loss: 0.000296 |\n",
      "| Train_epoch: 17 | Train_batch: 1050/1335 | Train_batch_loss: 0.000290 |\n",
      "| Train_epoch: 17 | Train_batch: 1100/1335 | Train_batch_loss: 0.000296 |\n",
      "| Train_epoch: 17 | Train_batch: 1150/1335 | Train_batch_loss: 0.000288 |\n",
      "| Train_epoch: 17 | Train_batch: 1200/1335 | Train_batch_loss: 0.000280 |\n",
      "| Train_epoch: 17 | Train_batch: 1250/1335 | Train_batch_loss: 0.000275 |\n",
      "| Train_epoch: 17 | Train_batch: 1300/1335 | Train_batch_loss: 0.000277 |\n",
      "| Train_epoch: 17 | Train_loss: 0.000273 | Train&Val_time: 8m 9s |\n",
      "                 | Valid_loss: 0.034851 | Valid_f1: 96.413357 | Valid_Acc: 99.809961% |\n",
      "\n",
      "*****  Train Epoch 18/20  *****\n",
      "| Train_epoch: 18 | Train_batch: 50/1335 | Train_batch_loss: 0.000322 |\n",
      "| Train_epoch: 18 | Train_batch: 100/1335 | Train_batch_loss: 0.000244 |\n",
      "| Train_epoch: 18 | Train_batch: 150/1335 | Train_batch_loss: 0.000235 |\n",
      "| Train_epoch: 18 | Train_batch: 200/1335 | Train_batch_loss: 0.000195 |\n",
      "| Train_epoch: 18 | Train_batch: 250/1335 | Train_batch_loss: 0.000229 |\n",
      "| Train_epoch: 18 | Train_batch: 300/1335 | Train_batch_loss: 0.000281 |\n",
      "| Train_epoch: 18 | Train_batch: 350/1335 | Train_batch_loss: 0.000270 |\n",
      "| Train_epoch: 18 | Train_batch: 400/1335 | Train_batch_loss: 0.000260 |\n",
      "| Train_epoch: 18 | Train_batch: 450/1335 | Train_batch_loss: 0.000254 |\n",
      "| Train_epoch: 18 | Train_batch: 500/1335 | Train_batch_loss: 0.000236 |\n",
      "| Train_epoch: 18 | Train_batch: 550/1335 | Train_batch_loss: 0.000229 |\n",
      "| Train_epoch: 18 | Train_batch: 600/1335 | Train_batch_loss: 0.000216 |\n",
      "| Train_epoch: 18 | Train_batch: 650/1335 | Train_batch_loss: 0.000230 |\n",
      "| Train_epoch: 18 | Train_batch: 700/1335 | Train_batch_loss: 0.000246 |\n",
      "| Train_epoch: 18 | Train_batch: 750/1335 | Train_batch_loss: 0.000240 |\n",
      "| Train_epoch: 18 | Train_batch: 800/1335 | Train_batch_loss: 0.000234 |\n",
      "| Train_epoch: 18 | Train_batch: 850/1335 | Train_batch_loss: 0.000234 |\n",
      "| Train_epoch: 18 | Train_batch: 900/1335 | Train_batch_loss: 0.000239 |\n",
      "| Train_epoch: 18 | Train_batch: 950/1335 | Train_batch_loss: 0.000238 |\n",
      "| Train_epoch: 18 | Train_batch: 1000/1335 | Train_batch_loss: 0.000245 |\n",
      "| Train_epoch: 18 | Train_batch: 1050/1335 | Train_batch_loss: 0.000238 |\n",
      "| Train_epoch: 18 | Train_batch: 1100/1335 | Train_batch_loss: 0.000237 |\n",
      "| Train_epoch: 18 | Train_batch: 1150/1335 | Train_batch_loss: 0.000240 |\n",
      "| Train_epoch: 18 | Train_batch: 1200/1335 | Train_batch_loss: 0.000249 |\n",
      "| Train_epoch: 18 | Train_batch: 1250/1335 | Train_batch_loss: 0.000244 |\n",
      "| Train_epoch: 18 | Train_batch: 1300/1335 | Train_batch_loss: 0.000240 |\n",
      "| Train_epoch: 18 | Train_loss: 0.000241 | Train&Val_time: 8m 12s |\n",
      "                 | Valid_loss: 0.033565 | Valid_f1: 96.339959 | Valid_Acc: 99.811144% |\n",
      "\n",
      "*****  Train Epoch 19/20  *****\n",
      "| Train_epoch: 19 | Train_batch: 50/1335 | Train_batch_loss: 0.000237 |\n",
      "| Train_epoch: 19 | Train_batch: 100/1335 | Train_batch_loss: 0.000221 |\n",
      "| Train_epoch: 19 | Train_batch: 150/1335 | Train_batch_loss: 0.000192 |\n",
      "| Train_epoch: 19 | Train_batch: 200/1335 | Train_batch_loss: 0.000206 |\n",
      "| Train_epoch: 19 | Train_batch: 250/1335 | Train_batch_loss: 0.000205 |\n",
      "| Train_epoch: 19 | Train_batch: 300/1335 | Train_batch_loss: 0.000213 |\n",
      "| Train_epoch: 19 | Train_batch: 350/1335 | Train_batch_loss: 0.000220 |\n",
      "| Train_epoch: 19 | Train_batch: 400/1335 | Train_batch_loss: 0.000214 |\n",
      "| Train_epoch: 19 | Train_batch: 450/1335 | Train_batch_loss: 0.000206 |\n",
      "| Train_epoch: 19 | Train_batch: 500/1335 | Train_batch_loss: 0.000193 |\n",
      "| Train_epoch: 19 | Train_batch: 550/1335 | Train_batch_loss: 0.000185 |\n",
      "| Train_epoch: 19 | Train_batch: 600/1335 | Train_batch_loss: 0.000178 |\n",
      "| Train_epoch: 19 | Train_batch: 650/1335 | Train_batch_loss: 0.000169 |\n",
      "| Train_epoch: 19 | Train_batch: 700/1335 | Train_batch_loss: 0.000173 |\n",
      "| Train_epoch: 19 | Train_batch: 750/1335 | Train_batch_loss: 0.000166 |\n",
      "| Train_epoch: 19 | Train_batch: 800/1335 | Train_batch_loss: 0.000158 |\n",
      "| Train_epoch: 19 | Train_batch: 850/1335 | Train_batch_loss: 0.000150 |\n",
      "| Train_epoch: 19 | Train_batch: 900/1335 | Train_batch_loss: 0.000173 |\n",
      "| Train_epoch: 19 | Train_batch: 950/1335 | Train_batch_loss: 0.000176 |\n",
      "| Train_epoch: 19 | Train_batch: 1000/1335 | Train_batch_loss: 0.000173 |\n",
      "| Train_epoch: 19 | Train_batch: 1050/1335 | Train_batch_loss: 0.000168 |\n",
      "| Train_epoch: 19 | Train_batch: 1100/1335 | Train_batch_loss: 0.000164 |\n",
      "| Train_epoch: 19 | Train_batch: 1150/1335 | Train_batch_loss: 0.000167 |\n",
      "| Train_epoch: 19 | Train_batch: 1200/1335 | Train_batch_loss: 0.000170 |\n",
      "| Train_epoch: 19 | Train_batch: 1250/1335 | Train_batch_loss: 0.000165 |\n",
      "| Train_epoch: 19 | Train_batch: 1300/1335 | Train_batch_loss: 0.000160 |\n",
      "| Train_epoch: 19 | Train_loss: 0.000163 | Train&Val_time: 7m 38s |\n",
      "                 | Valid_loss: 0.033926 | Valid_f1: 96.364385 | Valid_Acc: 99.814298% |\n",
      "\n",
      "*****  Train Epoch 20/20  *****\n",
      "| Train_epoch: 20 | Train_batch: 50/1335 | Train_batch_loss: 0.000110 |\n",
      "| Train_epoch: 20 | Train_batch: 100/1335 | Train_batch_loss: 0.000074 |\n",
      "| Train_epoch: 20 | Train_batch: 150/1335 | Train_batch_loss: 0.000092 |\n",
      "| Train_epoch: 20 | Train_batch: 200/1335 | Train_batch_loss: 0.000084 |\n",
      "| Train_epoch: 20 | Train_batch: 250/1335 | Train_batch_loss: 0.000091 |\n",
      "| Train_epoch: 20 | Train_batch: 300/1335 | Train_batch_loss: 0.000104 |\n",
      "| Train_epoch: 20 | Train_batch: 350/1335 | Train_batch_loss: 0.000125 |\n",
      "| Train_epoch: 20 | Train_batch: 400/1335 | Train_batch_loss: 0.000115 |\n",
      "| Train_epoch: 20 | Train_batch: 450/1335 | Train_batch_loss: 0.000105 |\n",
      "| Train_epoch: 20 | Train_batch: 500/1335 | Train_batch_loss: 0.000108 |\n",
      "| Train_epoch: 20 | Train_batch: 550/1335 | Train_batch_loss: 0.000103 |\n",
      "| Train_epoch: 20 | Train_batch: 600/1335 | Train_batch_loss: 0.000098 |\n",
      "| Train_epoch: 20 | Train_batch: 650/1335 | Train_batch_loss: 0.000094 |\n",
      "| Train_epoch: 20 | Train_batch: 700/1335 | Train_batch_loss: 0.000097 |\n",
      "| Train_epoch: 20 | Train_batch: 750/1335 | Train_batch_loss: 0.000100 |\n",
      "| Train_epoch: 20 | Train_batch: 800/1335 | Train_batch_loss: 0.000096 |\n",
      "| Train_epoch: 20 | Train_batch: 850/1335 | Train_batch_loss: 0.000096 |\n",
      "| Train_epoch: 20 | Train_batch: 900/1335 | Train_batch_loss: 0.000100 |\n",
      "| Train_epoch: 20 | Train_batch: 950/1335 | Train_batch_loss: 0.000105 |\n",
      "| Train_epoch: 20 | Train_batch: 1000/1335 | Train_batch_loss: 0.000101 |\n",
      "| Train_epoch: 20 | Train_batch: 1050/1335 | Train_batch_loss: 0.000107 |\n",
      "| Train_epoch: 20 | Train_batch: 1100/1335 | Train_batch_loss: 0.000104 |\n",
      "| Train_epoch: 20 | Train_batch: 1150/1335 | Train_batch_loss: 0.000101 |\n",
      "| Train_epoch: 20 | Train_batch: 1200/1335 | Train_batch_loss: 0.000104 |\n",
      "| Train_epoch: 20 | Train_batch: 1250/1335 | Train_batch_loss: 0.000102 |\n",
      "| Train_epoch: 20 | Train_batch: 1300/1335 | Train_batch_loss: 0.000101 |\n",
      "| Train_epoch: 20 | Train_loss: 0.000102 | Train&Val_time: 7m 43s |\n",
      "                 | Valid_loss: 0.035141 | Valid_f1: 96.320703 | Valid_Acc: 99.814298% |\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "if args.store_dir is not None:\n",
    "    model = BertForSequenceTagging.from_pretrained(model_params_dir)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, args.max_epoch_num +1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    print('\\n*****',' Train Epoch {0}/{1} '.format(epoch, args.max_epoch_num), '*****')\n",
    "    \n",
    "    train_steps = args.train_size // args.batch_size\n",
    "    val_steps = args.val_size // args.batch_size\n",
    "    \n",
    "    train_data_iterator = data_loader.data_iterator(train, shuffle=True)\n",
    "    val_data_iterator = data_loader.data_iterator(val, shuffle=True)\n",
    "    \n",
    "    # Train step\n",
    "    model.train()\n",
    "    epoch_loss, epoch_avg_loss = 0.0, 0.0\n",
    "\n",
    "    for batch in range(1, train_steps+1):\n",
    "        batch_data, batch_token_starts, batch_tags = next(train_data_iterator)\n",
    "        batch_masks = batch_data.gt(0)\n",
    "    \n",
    "        loss = model((batch_data, batch_token_starts), token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)[0]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=args.clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_avg_loss = epoch_loss / (batch+1)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print('| Train_epoch: {0} | Train_batch: {1}/{2} | Train_batch_loss: {3:4f} |'.format(epoch, batch, train_steps, epoch_avg_loss))\n",
    "    \n",
    "    # Val step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    true_tags, pred_tags = [], []\n",
    "    \n",
    "    for _ in range(val_steps):\n",
    "        batch_data, batch_token_starts, batch_tags = next(val_data_iterator)\n",
    "        batch_masks = batch_data.gt(0)\n",
    "        \n",
    "        loss = model((batch_data, batch_token_starts), token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)[0]\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        batch_outs = model((batch_data, batch_token_starts), token_type_ids=None, attention_mask=batch_masks)[0]\n",
    "        batch_outs = batch_outs.detach().cpu().numpy()\n",
    "        batch_tags = batch_tags.to('cpu').numpy()\n",
    "        \n",
    "        pred_tags.extend([[args.idx2tag.get(idx) for idx in indices] for indices in np.argmax(batch_outs, axis=2)])\n",
    "        true_tags.extend([[args.idx2tag.get(idx) if idx != -1 else 'O' for idx in indices] for indices in batch_tags])\n",
    "    \n",
    "    assert len(pred_tags) == len(true_tags)\n",
    "\n",
    "    f1 = f1_score(true_tags, pred_tags)\n",
    "    loss = val_loss / val_steps\n",
    "    acc = accuracy_score(true_tags, pred_tags)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_mins, epoch_secs = int(epoch_time/60), int(epoch_time%60)\n",
    "    \n",
    "    print('| Train_epoch: {0} | Train_loss: {2:4f} | Train&Val_time: {3}m {4}s |'.format(epoch, args.max_epoch_num, epoch_avg_loss, epoch_mins, epoch_secs))\n",
    "    print('                 | Valid_loss: {0:4f} | Valid_f1: {1:4f} | Valid_Acc: {2:2f}% |'.format(loss, f1, 100*acc))\n",
    "\n",
    "    if f1 - best_val_f1 >= 0.1:\n",
    "        best_val_f1 = f1\n",
    "        \n",
    "        model_path = model_params_dir+'/pytorch_model.pt'\n",
    "        config_path = model_params_dir+'/config.json'\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path, _use_new_zipfile_serialization=False)\n",
    "        model.config.to_json_file(config_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
