{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='msra')\n",
    "parser.add_argument('--val_ratio', default=0.05)\n",
    "parser.add_argument('--seed', default=1234)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "data_dir = 'data/' + args.dataset\n",
    "path_train = data_dir + '/train_bio'\n",
    "path_val = data_dir + '/val_bio'\n",
    "path_test = data_dir + '/test_bio'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_dataset):\n",
    "    \"\"\"Load dataset into memory from text file\"\"\"\n",
    "    dataset = []\n",
    "    with open(path_dataset) as f:\n",
    "        words, tags = [], []\n",
    "        # Each line of the file corresponds to one word and tag\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                line = line.strip('\\n')\n",
    "                if len(line.split()) > 1:\n",
    "                    word = line.split()[0]\n",
    "                    tag = line.split()[-1]\n",
    "                else:\n",
    "                    continue\n",
    "                try:\n",
    "                    if len(word) > 0 and len(tag) > 0:\n",
    "                        word, tag = str(word), str(tag)\n",
    "                        words.append(word)\n",
    "                        tags.append(tag)\n",
    "                except Exception as e:\n",
    "                    print('An exception was raised, skipping a word: {}'.format(e))\n",
    "            else:\n",
    "                if len(words) > 0:\n",
    "                    assert len(words) == len(tags)\n",
    "                    dataset.append((words, tags))\n",
    "                    words, tags = [], []\n",
    "    return dataset\n",
    "\n",
    "def save_dataset(dataset, save_dir):\n",
    "    \"\"\"Write sentences.txt and tags.txt files in save_dir from dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: ([([\"a\", \"cat\"], [\"O\", \"O\"]), ...])\n",
    "        save_dir: (string)\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    print('Saving in {}...'.format(save_dir))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Export the dataset\n",
    "    with open(os.path.join(save_dir, 'sentences.txt'), 'w') as file_sentences, \\\n",
    "        open(os.path.join(save_dir, 'tags.txt'), 'w') as file_tags:\n",
    "        for words, tags in dataset:\n",
    "            file_sentences.write('{}\\n'.format(' '.join(words)))\n",
    "            file_tags.write('{}\\n'.format(' '.join(tags)))\n",
    "    print('- done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train/val/test data, data_train/val/tesi with list type is\n",
    "# the result. 0-dimension is text and 1-dimension is tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42750 2250 3442\n",
      "Saving in data/msra/train...\n",
      "- done.\n",
      "Saving in data/msra/val...\n",
      "- done.\n",
      "Saving in data/msra/test...\n",
      "- done.\n"
     ]
    }
   ],
   "source": [
    "data_train = load_dataset(path_train)\n",
    "data_test = load_dataset(path_test)\n",
    "\n",
    "\n",
    "if os.path.exists(path_val):\n",
    "    data_val = load_dataset(path_val)\n",
    "else:\n",
    "    total_train_len = len(data_train)\n",
    "    split_val_len = int(total_train_len * args.val_ratio)\n",
    "    order = list(range(total_train_len))\n",
    "    # Set seed for random processing\n",
    "    random.seed(args.seed)\n",
    "    random.shuffle(order)\n",
    "    \n",
    "    # Split trainsets into train and val with split-val-ratio\n",
    "    data_val = [data_train[idx] for idx in order[:split_val_len]]\n",
    "    data_train = [data_train[idx] for idx in order[split_val_len:]]\n",
    "\n",
    "print(len(data_train), len(data_val), len(data_test))\n",
    "save_dataset(data_train, data_dir + '/train')\n",
    "save_dataset(data_val, data_dir + '/val')\n",
    "save_dataset(data_test, data_dir + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_tags(data_dir, tags_file):\n",
    "    data_types = ['train', 'val', 'test']\n",
    "    tags = set()\n",
    "\n",
    "    for data_type in data_types:\n",
    "        tags_path = os.path.join(data_dir, data_type, 'tags.txt')\n",
    "        with open(tags_path, 'r') as file:\n",
    "            for line in file:\n",
    "                tag_seq = filter(len, line.strip().split(' '))\n",
    "                tags.update(list(tag_seq))\n",
    "\n",
    "    tags = sorted(tags)\n",
    "    with open(tags_file, 'w') as file:\n",
    "        file.write('\\n'.join(tags))\n",
    "    return tags\n",
    "tags = build_tags(data_dir, data_dir + '/tags.txt')\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
