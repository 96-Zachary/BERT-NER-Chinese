{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import utils as utils\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='msra')\n",
    "parser.add_argument('--seed', default=1234)\n",
    "parser.add_argument('--store_dir', default=None)\n",
    "\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "parser.add_argument('--max_len', default=128)\n",
    "parser.add_argument('--patience', default=0.02)\n",
    "parser.add_argument('--patience_num', default=5)\n",
    "\n",
    "parser.add_argument('--device', default=None)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_params_dir = 'experiments/' + args.dataset\n",
    "json_path = os.path.join(model_params_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "params.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "params.seed = args.seed\n",
    "\n",
    "data_dir = 'data/' + args.dataset\n",
    "if args.dataset == 'msra':\n",
    "    bert_class = 'bert-base-chinese'\n",
    "else:\n",
    "    bert_class = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f825c82cf50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for code\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# params.seed = args.seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data_dir, bert_class, args, token_pad_idx=0, tag_pad_idx=1):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = args.batch_size\n",
    "        self.max_len = args.max_len\n",
    "        self.device = args.device\n",
    "        self.seed = args.seed\n",
    "        self.token_pad_idx = token_pad_idx\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        \n",
    "        tags = self.load_tags()\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "        self.idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "        \n",
    "        args.tag2idx = self.tag2idx\n",
    "        args.idx2tag = self.idx2tag\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_class, do_lower_case=False)\n",
    "        \n",
    "    def load_tags(self):\n",
    "        tags  = []\n",
    "        tags_path = os.path.join(self.data_dir, 'tags.txt')\n",
    "        \n",
    "        with open(tags_path, 'r') as file:\n",
    "            for tag in file:\n",
    "                tags.append(tag.strip())\n",
    "        return tags\n",
    "    \n",
    "    def load_sentence_tags(self, sentence_path, tags_path, data={}):\n",
    "        sentences = []\n",
    "        tags = []\n",
    "        \n",
    "        with open(sentence_path, 'r') as file:\n",
    "            for line in file:\n",
    "                tokens = line.strip().split(' ')\n",
    "                subwords = list(map(self.tokenizer.tokenize, tokens))\n",
    "                subword_lengths = list(map(len, subwords))\n",
    "                subwords = ['[CLS]'] + [item for indices in subwords for item in indices]\n",
    "                # indice words except [CLS]\n",
    "                token_start_idxs = list(range(1,len(subwords)))\n",
    "                \n",
    "                bert_tokens = self.tokenizer.convert_tokens_to_ids(subwords)\n",
    "                sentences.append((bert_tokens, token_start_idxs))\n",
    "                # len(bert_tokens) - len(token_start_idxs) = 1\n",
    "  \n",
    "                \n",
    "        if tags_path != None:\n",
    "            with open(tags_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    tag_seq = [self.tag2idx.get(tag) for tag in line.strip().split(' ')]\n",
    "                    tags.append(tag_seq)\n",
    "            \n",
    "            # Check the corresponding between sentences and tags\n",
    "            assert len(sentences) == len(tags)\n",
    "            for i in range(len(tags)):\n",
    "                assert len(tags[i]) == len(sentences[i][0])-1\n",
    "            data['tags'] = tags\n",
    "            \n",
    "        data['sentences'] = sentences\n",
    "        data['size'] = len(sentences)\n",
    "        \n",
    "    def load_data(self, data_class):\n",
    "        data = {}\n",
    "        \n",
    "        if data_class in ['train', 'val', 'test']:\n",
    "            sentence_path = os.path.join(data_dir, data_class, 'sentences.txt')\n",
    "            tags_path = os.path.join(data_dir, data_class, 'tags.txt')\n",
    "            \n",
    "            self.load_sentence_tags(sentence_path, tags_path, data)\n",
    "        \n",
    "        elif data_class == 'interactive':\n",
    "            sentence_path = os.path.join(data_dir, data_class, 'sentences.txt')\n",
    "            tags_path=None\n",
    "            self.load_sentence_tags(sentence_path, tags_path, data)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No data in train/val/test or interactve!\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def data_iterator(self, data, shuffle=False):\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(self.seed)\n",
    "            random.shuffle(order)\n",
    "        InterModel = False if 'tags' in data else True\n",
    "        \n",
    "        if data['size'] % self.batch_size == 0:\n",
    "            BATCH_SIZE = data['size'] // self.batch_size\n",
    "        else:\n",
    "            BATCH_SIZE = data['size'] // self.batch_size + 1\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            # fetch sentences and tags\n",
    "            if i * self.batch_size < data['size'] < (i+1) * self.batch_size:\n",
    "                sentences = [data['sentences'][idx] for idx in order[i*self.batch_size:]]\n",
    "                if not InterModel:\n",
    "                    tags = [data['tags'][idx] for idx in order[i*self.batch_size:]]\n",
    "            else:\n",
    "                sentences = [data['sentences'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "                if not InterModel:\n",
    "                    tags = [data['tags'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "\n",
    "            # batch length\n",
    "            batch_len = len(sentences)\n",
    "\n",
    "            # compute length of longest sentence in batch\n",
    "            batch_max_subwords_len = max([len(s[0]) for s in sentences])\n",
    "            max_subwords_len = min(batch_max_subwords_len, self.max_len)\n",
    "            max_token_len = 0\n",
    "\n",
    "\n",
    "            # prepare a numpy array with the data, initialising the data with pad_idx\n",
    "            batch_data = self.token_pad_idx * np.ones((batch_len, max_subwords_len))\n",
    "            batch_token_starts = []\n",
    "            \n",
    "            # copy the data to the numpy array\n",
    "            for j in range(batch_len):\n",
    "                cur_subwords_len = len(sentences[j][0])\n",
    "                if cur_subwords_len <= max_subwords_len:\n",
    "                    batch_data[j][:cur_subwords_len] = sentences[j][0]\n",
    "                else:\n",
    "                    batch_data[j] = sentences[j][0][:max_subwords_len]\n",
    "                token_start_idx = sentences[j][-1]\n",
    "                token_starts = np.zeros(max_subwords_len)\n",
    "                token_starts[[idx for idx in token_start_idx if idx < max_subwords_len]] = 1\n",
    "                batch_token_starts.append(token_starts)\n",
    "                max_token_len = max(int(sum(token_starts)), max_token_len)\n",
    "            \n",
    "            if not InterModel:\n",
    "                batch_tags = self.tag_pad_idx * np.ones((batch_len, max_token_len))\n",
    "                for j in range(batch_len):\n",
    "                    cur_tags_len = len(tags[j])  \n",
    "                    if cur_tags_len <= max_token_len:\n",
    "                        batch_tags[j][:cur_tags_len] = tags[j]\n",
    "                    else:\n",
    "                        batch_tags[j] = tags[j][:max_token_len]\n",
    "            \n",
    "            # since all data are indices, we convert them to torch LongTensors\n",
    "            batch_data = torch.tensor(batch_data, dtype=torch.long)\n",
    "            batch_token_starts = torch.tensor(batch_token_starts, dtype=torch.long)\n",
    "            if not InterModel:\n",
    "                batch_tags = torch.tensor(batch_tags, dtype=torch.long)\n",
    "\n",
    "            # shift tensors to GPU if available\n",
    "            batch_data, batch_token_starts = batch_data.to(self.device), batch_token_starts.to(self.device)\n",
    "            if not InterModel:\n",
    "                batch_tags = batch_tags.to(self.device)\n",
    "                yield batch_data, batch_token_starts, batch_tags\n",
    "            else:\n",
    "                yield batch_data, batch_token_starts\n",
    "\n",
    "data_loader = DataLoader(data_dir, bert_class, args, token_pad_idx=0, tag_pad_idx=-1)\n",
    "idx2tag = data_loader.idx2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceTagging(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SequenceTagger import BertForSequenceTagging\n",
    "\n",
    "model = BertForSequenceTagging.from_pretrained(model_params_dir)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Function\n",
    "from metrics import get_entities\n",
    "\n",
    "def BertNerResponse(model, querystring):\n",
    "    # gain word-level sentence\n",
    "    querystring = [i for i in querystring]\n",
    "    \n",
    "    with open('data/' + args.dataset + '/interactive/sentences.txt', 'w') as f:\n",
    "        f.write(' '.join(querystring))\n",
    "    \n",
    "    inter_data = data_loader.load_data('interactive')\n",
    "    inter_data_iterator = data_loader.data_iterator(inter_data, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_data, batch_token_starts = next(inter_data_iterator)\n",
    "    batch_masks = batch_data.gt(0)\n",
    "    \n",
    "    batch_outs = model((batch_data, batch_token_starts), token_type_ids=None, attention_mask=batch_masks)[0]\n",
    "    batch_outs = batch_outs.detach().cpu().numpy()\n",
    "    \n",
    "    pred_tags = []\n",
    "    pred_tags.extend([[idx2tag[idx] for idx in indices] for indices in np.argmax(batch_outs, axis=2)]) \n",
    "    result = get_entities(pred_tags)\n",
    "    \n",
    "    res = []\n",
    "    for item in result:\n",
    "        res.append((''.join(querystring[item[1]:item[2]+1]), item[0]))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:郑雨柔是个小傻瓜\n",
      "[('郑雨柔', 'PER')]\n",
      "Input:无时无刻，天安门广场飘扬的五星红旗都是我们的希望\n",
      "[('天安门广场', 'LOC')]\n",
      "Input:exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input('Input:')\n",
    "    if query == 'exit':\n",
    "        break\n",
    "    print(BertNerResponse(model, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
